* 
* ==> Audit <==
* |--------------|--------------------------------|----------|---------|---------|-------------------------------|-------------------------------|
|   Command    |              Args              | Profile  |  User   | Version |          Start Time           |           End Time            |
|--------------|--------------------------------|----------|---------|---------|-------------------------------|-------------------------------|
| update-check |                                | minikube | kuldeep | v1.25.2 | Wed, 27 Jul 2022 12:37:18 IST | Wed, 27 Jul 2022 12:37:19 IST |
| update-check |                                | minikube | kuldeep | v1.25.2 | Thu, 24 Nov 2022 19:24:14 GMT | Thu, 24 Nov 2022 19:24:14 GMT |
| update-check |                                | minikube | kuldeep | v1.25.2 | Sun, 02 Apr 2023 19:44:06 BST | Sun, 02 Apr 2023 19:44:07 BST |
| update-check |                                | minikube | kuldeep | v1.25.2 | Sun, 02 Apr 2023 20:08:23 BST | Sun, 02 Apr 2023 20:08:23 BST |
| update-check |                                | minikube | kuldeep | v1.25.2 | Fri, 21 Apr 2023 16:12:15 BST | Fri, 21 Apr 2023 16:12:15 BST |
| update-check |                                | minikube | kuldeep | v1.30.1 | 22 Apr 23 13:10 BST           | 22 Apr 23 13:10 BST           |
| update-check |                                | minikube | kuldeep | v1.30.1 | 23 Apr 23 20:47 BST           | 23 Apr 23 20:47 BST           |
| update-check |                                | minikube | kuldeep | v1.30.1 | 24 Apr 23 13:58 BST           | 24 Apr 23 13:58 BST           |
| update-check |                                | minikube | kuldeep | v1.30.1 | 26 Apr 23 21:09 BST           | 26 Apr 23 21:09 BST           |
| update-check |                                | minikube | kuldeep | v1.30.1 | 30 Apr 23 11:15 BST           | 30 Apr 23 11:15 BST           |
| update-check |                                | minikube | kuldeep | v1.30.1 | 01 May 23 14:03 BST           | 01 May 23 14:03 BST           |
| update-check |                                | minikube | kuldeep | v1.30.1 | 03 May 23 01:09 BST           | 03 May 23 01:09 BST           |
| update-check |                                | minikube | kuldeep | v1.30.1 | 04 May 23 23:27 BST           | 04 May 23 23:27 BST           |
| update-check |                                | minikube | kuldeep | v1.30.1 | 06 May 23 13:37 BST           | 06 May 23 13:37 BST           |
| update-check |                                | minikube | kuldeep | v1.30.1 | 11 May 23 11:51 BST           | 11 May 23 11:51 BST           |
| update-check |                                | minikube | kuldeep | v1.30.1 | 05 Jun 23 19:09 BST           | 05 Jun 23 19:09 BST           |
| update-check |                                | minikube | kuldeep | v1.30.1 | 24 Jun 23 16:57 BST           | 24 Jun 23 16:57 BST           |
| update-check |                                | minikube | kuldeep | v1.30.1 | 06 Jul 23 08:36 BST           | 06 Jul 23 08:36 BST           |
| update-check |                                | minikube | kuldeep | v1.30.1 | 08 Jul 23 10:39 BST           | 08 Jul 23 10:39 BST           |
| update-check |                                | minikube | kuldeep | v1.30.1 | 09 Jul 23 10:18 BST           | 09 Jul 23 10:18 BST           |
| update-check |                                | minikube | kuldeep | v1.30.1 | 11 Jul 23 09:54 BST           | 11 Jul 23 09:54 BST           |
| update-check |                                | minikube | kuldeep | v1.30.1 | 18 Jul 23 11:19 BST           | 18 Jul 23 11:19 BST           |
| update-check |                                | minikube | kuldeep | v1.30.1 | 25 Jul 23 11:54 BST           | 25 Jul 23 11:54 BST           |
| update-check |                                | minikube | kuldeep | v1.30.1 | 05 Aug 23 09:00 BST           | 05 Aug 23 09:00 BST           |
| start        | --driver=none                  | minikube | kuldeep | v1.31.1 | 06 Aug 23 09:01 BST           |                               |
| start        |                                | minikube | kuldeep | v1.31.1 | 06 Aug 23 09:01 BST           | 06 Aug 23 09:07 BST           |
| start        | --driver=none                  | minikube | kuldeep | v1.31.1 | 06 Aug 23 09:36 BST           |                               |
| help         |                                | minikube | kuldeep | v1.31.1 | 06 Aug 23 09:37 BST           | 06 Aug 23 09:37 BST           |
| dashboard    |                                | minikube | kuldeep | v1.31.1 | 06 Aug 23 09:37 BST           |                               |
| addons       | enable metrics-server          | minikube | kuldeep | v1.31.1 | 06 Aug 23 09:38 BST           | 06 Aug 23 09:38 BST           |
| service      | k8s-spring-app-service --url   | minikube | kuldeep | v1.31.1 | 08 Aug 23 17:12 BST           |                               |
| docker-env   |                                | minikube | kuldeep | v1.31.1 | 08 Aug 23 17:16 BST           | 08 Aug 23 17:16 BST           |
| start        |                                | minikube | kuldeep | v1.31.1 | 08 Aug 23 17:18 BST           | 08 Aug 23 17:18 BST           |
| dashboard    |                                | minikube | kuldeep | v1.31.1 | 08 Aug 23 17:18 BST           |                               |
| dashboard    |                                | minikube | kuldeep | v1.31.1 | 08 Aug 23 17:22 BST           |                               |
| service      | k8s-spring-app-service --url   | minikube | kuldeep | v1.31.1 | 08 Aug 23 17:39 BST           | 08 Aug 23 17:49 BST           |
| start        | --driver=hyperkit              | minikube | kuldeep | v1.31.1 | 08 Aug 23 17:44 BST           |                               |
| delete       |                                | minikube | kuldeep | v1.31.1 | 08 Aug 23 17:45 BST           | 08 Aug 23 17:45 BST           |
| start        | --driver=hyperkit              | minikube | kuldeep | v1.31.1 | 08 Aug 23 17:45 BST           |                               |
| start        | --driver=virtualbox            | minikube | kuldeep | v1.31.1 | 08 Aug 23 17:45 BST           |                               |
| start        | --driver=hyperkit              | minikube | kuldeep | v1.31.1 | 08 Aug 23 17:46 BST           |                               |
| start        | --driver qemu --network        | minikube | kuldeep | v1.31.1 | 08 Aug 23 17:48 BST           |                               |
|              | socket_vmnet                   |          |         |         |                               |                               |
| start        | --driver qemu --network        | minikube | kuldeep | v1.31.1 | 08 Aug 23 17:51 BST           | 08 Aug 23 17:53 BST           |
|              | socket_vmnet                   |          |         |         |                               |                               |
| dashboard    |                                | minikube | kuldeep | v1.31.1 | 08 Aug 23 17:55 BST           |                               |
| addons       | enable metrics-server          | minikube | kuldeep | v1.31.1 | 08 Aug 23 17:56 BST           | 08 Aug 23 17:56 BST           |
| dashboard    |                                | minikube | kuldeep | v1.31.1 | 08 Aug 23 17:56 BST           |                               |
| service      | k8s-spring-app-service --url   | minikube | kuldeep | v1.31.1 | 08 Aug 23 17:59 BST           | 08 Aug 23 17:59 BST           |
| ip           |                                | minikube | kuldeep | v1.31.1 | 08 Aug 23 18:05 BST           | 08 Aug 23 18:05 BST           |
| start        |                                | minikube | kuldeep | v1.31.1 | 16 Aug 23 11:53 BST           | 16 Aug 23 11:54 BST           |
| addons       | enable metrics-server          | minikube | kuldeep | v1.31.1 | 16 Aug 23 11:55 BST           | 16 Aug 23 11:55 BST           |
| dashboard    |                                | minikube | kuldeep | v1.31.1 | 16 Aug 23 11:55 BST           |                               |
| start        | --driver qemu --network        | minikube | kuldeep | v1.31.1 | 16 Aug 23 16:50 BST           | 16 Aug 23 16:51 BST           |
|              | socket_vmnet                   |          |         |         |                               |                               |
| dashboard    |                                | minikube | kuldeep | v1.31.1 | 16 Aug 23 16:52 BST           |                               |
| service      | k8s-spring-app-service --url   | minikube | kuldeep | v1.31.1 | 16 Aug 23 16:53 BST           |                               |
| service      | expense-service-app --url      | minikube | kuldeep | v1.31.1 | 16 Aug 23 16:54 BST           | 16 Aug 23 16:54 BST           |
| service      | user-app-service --url         | minikube | kuldeep | v1.31.1 | 16 Aug 23 16:56 BST           |                               |
| service      | user-app-service --url         | minikube | kuldeep | v1.31.1 | 16 Aug 23 16:59 BST           |                               |
|--------------|--------------------------------|----------|---------|---------|-------------------------------|-------------------------------|

* 
* ==> Last Start <==
* Log file created at: 2023/08/16 16:50:24
Running on machine: Kuldeeps-MacBook-Pro
Binary: Built with gc go1.20.6 for darwin/arm64
Log line format: [IWEF]mmdd hh:mm:ss.uuuuuu threadid file:line] msg
I0816 16:50:24.347788   91938 out.go:296] Setting OutFile to fd 1 ...
I0816 16:50:24.348510   91938 out.go:348] isatty.IsTerminal(1) = true
I0816 16:50:24.348512   91938 out.go:309] Setting ErrFile to fd 2...
I0816 16:50:24.348515   91938 out.go:348] isatty.IsTerminal(2) = true
I0816 16:50:24.349167   91938 root.go:338] Updating PATH: /Users/kuldeep/.minikube/bin
I0816 16:50:24.351139   91938 out.go:303] Setting JSON to false
I0816 16:50:24.375541   91938 start.go:128] hostinfo: {"hostname":"Kuldeeps-MacBook-Pro.local","uptime":16085,"bootTime":1692184939,"procs":337,"os":"darwin","platform":"darwin","platformFamily":"Standalone Workstation","platformVersion":"13.4.1","kernelVersion":"22.5.0","kernelArch":"arm64","virtualizationSystem":"","virtualizationRole":"","hostId":"cfa11bcc-db1a-5d5d-bf57-c2681822d472"}
W0816 16:50:24.375652   91938 start.go:136] gopshost.Virtualization returned error: not implemented yet
I0816 16:50:24.381921   91938 out.go:177] üòÑ  minikube v1.31.1 on Darwin 13.4.1 (arm64)
I0816 16:50:24.390236   91938 notify.go:220] Checking for updates...
I0816 16:50:24.390562   91938 config.go:182] Loaded profile config "minikube": Driver=qemu2, ContainerRuntime=docker, KubernetesVersion=v1.27.3
I0816 16:50:24.391905   91938 driver.go:373] Setting default libvirt URI to qemu:///system
I0816 16:50:24.400666   91938 out.go:177] ‚ú®  Using the qemu2 driver based on existing profile
I0816 16:50:24.409639   91938 start.go:298] selected driver: qemu2
I0816 16:50:24.409655   91938 start.go:898] validating driver "qemu2" against &{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO:https://storage.googleapis.com/minikube/iso/minikube-v1.31.0-arm64.iso KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.40@sha256:8cadf23777709e43eca447c47a45f5a4635615129267ce025193040ec92a1631 Memory:2200 CPUs:2 DiskSize:20000 VMDriver: Driver:qemu2 HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:0 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.27.3 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:} Nodes:[{Name: IP:192.168.105.2 Port:8443 KubernetesVersion:v1.27.3 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[dashboard:true default-storageclass:true metrics-server:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network:socket_vmnet Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/Users:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath:/opt/homebrew/opt/socket_vmnet/bin/socket_vmnet_client SocketVMnetPath:/opt/homebrew/var/run/socket_vmnet StaticIP: SSHAuthSock: SSHAgentPID:0}
I0816 16:50:24.409773   91938 start.go:909] status for qemu2: {Installed:true Healthy:true Running:true NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
I0816 16:50:24.410160   91938 cni.go:84] Creating CNI manager for ""
I0816 16:50:24.410183   91938 cni.go:158] "qemu2" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0816 16:50:24.410336   91938 start_flags.go:319] config:
{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO:https://storage.googleapis.com/minikube/iso/minikube-v1.31.0-arm64.iso KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.40@sha256:8cadf23777709e43eca447c47a45f5a4635615129267ce025193040ec92a1631 Memory:2200 CPUs:2 DiskSize:20000 VMDriver: Driver:qemu2 HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:0 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.27.3 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:} Nodes:[{Name: IP:192.168.105.2 Port:8443 KubernetesVersion:v1.27.3 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[dashboard:true default-storageclass:true metrics-server:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network:socket_vmnet Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/Users:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath:/opt/homebrew/opt/socket_vmnet/bin/socket_vmnet_client SocketVMnetPath:/opt/homebrew/var/run/socket_vmnet StaticIP: SSHAuthSock: SSHAgentPID:0}
I0816 16:50:24.417252   91938 iso.go:125] acquiring lock: {Name:mk747b11c255c59ff88d688f04677a58be1f802b Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I0816 16:50:24.426747   91938 out.go:177] üëç  Starting control plane node minikube in cluster minikube
I0816 16:50:24.430600   91938 preload.go:132] Checking if preload exists for k8s version v1.27.3 and runtime docker
I0816 16:50:24.430643   91938 preload.go:148] Found local preload: /Users/kuldeep/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.27.3-docker-overlay2-arm64.tar.lz4
I0816 16:50:24.430792   91938 cache.go:57] Caching tarball of preloaded images
I0816 16:50:24.430935   91938 preload.go:174] Found /Users/kuldeep/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.27.3-docker-overlay2-arm64.tar.lz4 in cache, skipping download
I0816 16:50:24.430940   91938 cache.go:60] Finished verifying existence of preloaded tar for  v1.27.3 on docker
I0816 16:50:24.431009   91938 profile.go:148] Saving config to /Users/kuldeep/.minikube/profiles/minikube/config.json ...
I0816 16:50:24.431669   91938 start.go:365] acquiring machines lock for minikube: {Name:mk6f56675c166f902cc652cad8dbdd926d32510c Clock:{} Delay:500ms Timeout:13m0s Cancel:<nil>}
I0816 16:50:24.431733   91938 start.go:369] acquired machines lock for "minikube" in 57.375¬µs
I0816 16:50:24.431740   91938 start.go:96] Skipping create...Using existing machine configuration
I0816 16:50:24.431761   91938 fix.go:54] fixHost starting: 
I0816 16:50:24.431903   91938 fix.go:102] recreateIfNeeded on minikube: state=Stopped err=<nil>
W0816 16:50:24.431911   91938 fix.go:128] unexpected machine state, will restart: <nil>
I0816 16:50:24.436721   91938 out.go:177] üîÑ  Restarting existing qemu2 VM for "minikube" ...
I0816 16:50:24.444915   91938 main.go:141] libmachine: executing: /opt/homebrew/opt/socket_vmnet/bin/socket_vmnet_client /opt/homebrew/var/run/socket_vmnet qemu-system-aarch64 -M virt,highmem=off -cpu host -drive file=/opt/homebrew/opt/qemu/share/qemu/edk2-aarch64-code.fd,readonly=on,format=raw,if=pflash -display none -accel hvf -m 2200 -smp 2 -boot d -cdrom /Users/kuldeep/.minikube/machines/minikube/boot2docker.iso -qmp unix:/Users/kuldeep/.minikube/machines/minikube/monitor,server,nowait -pidfile /Users/kuldeep/.minikube/machines/minikube/qemu.pid -device virtio-net-pci,netdev=net0,mac=fe:22:3c:db:71:f5 -netdev socket,id=net0,fd=3 -daemonize /Users/kuldeep/.minikube/machines/minikube/disk.qcow2
I0816 16:50:24.610812   91938 main.go:141] libmachine: STDOUT: 
I0816 16:50:24.610834   91938 main.go:141] libmachine: STDERR: 
I0816 16:50:24.610837   91938 main.go:141] libmachine: Attempt 0
I0816 16:50:24.610859   91938 main.go:141] libmachine: Searching for fe:22:3c:db:71:f5 in /var/db/dhcpd_leases ...
I0816 16:50:24.611170   91938 main.go:141] libmachine: Found 1 entries in /var/db/dhcpd_leases!
I0816 16:50:24.611184   91938 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.2 HWAddress:fe:22:3c:db:71:f5 ID:1,fe:22:3c:db:71:f5 Lease:0x64ddfc3f}
I0816 16:50:24.611187   91938 main.go:141] libmachine: Found match: fe:22:3c:db:71:f5
I0816 16:50:24.611202   91938 main.go:141] libmachine: IP: 192.168.105.2
I0816 16:50:24.611205   91938 main.go:141] libmachine: Waiting for VM to start (ssh -p 22 docker@192.168.105.2)...
I0816 16:50:59.651113   91938 profile.go:148] Saving config to /Users/kuldeep/.minikube/profiles/minikube/config.json ...
I0816 16:50:59.652570   91938 machine.go:88] provisioning docker machine ...
I0816 16:50:59.653527   91938 buildroot.go:166] provisioning hostname "minikube"
I0816 16:50:59.659030   91938 main.go:141] libmachine: Using SSH client type: native
I0816 16:50:59.665177   91938 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x10451b4b0] 0x10451df10 <nil>  [] 0s} 192.168.105.2 22 <nil> <nil>}
I0816 16:50:59.665292   91938 main.go:141] libmachine: About to run SSH command:
sudo hostname minikube && echo "minikube" | sudo tee /etc/hostname
I0816 16:50:59.767922   91938 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I0816 16:50:59.768246   91938 main.go:141] libmachine: Using SSH client type: native
I0816 16:50:59.768503   91938 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x10451b4b0] 0x10451df10 <nil>  [] 0s} 192.168.105.2 22 <nil> <nil>}
I0816 16:50:59.768509   91938 main.go:141] libmachine: About to run SSH command:

		if ! grep -xq '.*\sminikube' /etc/hosts; then
			if grep -xq '127.0.1.1\s.*' /etc/hosts; then
				sudo sed -i 's/^127.0.1.1\s.*/127.0.1.1 minikube/g' /etc/hosts;
			else 
				echo '127.0.1.1 minikube' | sudo tee -a /etc/hosts; 
			fi
		fi
I0816 16:50:59.844071   91938 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I0816 16:50:59.844079   91938 buildroot.go:172] set auth options {CertDir:/Users/kuldeep/.minikube CaCertPath:/Users/kuldeep/.minikube/certs/ca.pem CaPrivateKeyPath:/Users/kuldeep/.minikube/certs/ca-key.pem CaCertRemotePath:/etc/docker/ca.pem ServerCertPath:/Users/kuldeep/.minikube/machines/server.pem ServerKeyPath:/Users/kuldeep/.minikube/machines/server-key.pem ClientKeyPath:/Users/kuldeep/.minikube/certs/key.pem ServerCertRemotePath:/etc/docker/server.pem ServerKeyRemotePath:/etc/docker/server-key.pem ClientCertPath:/Users/kuldeep/.minikube/certs/cert.pem ServerCertSANs:[] StorePath:/Users/kuldeep/.minikube}
I0816 16:50:59.844329   91938 buildroot.go:174] setting up certificates
I0816 16:50:59.845164   91938 provision.go:83] configureAuth start
I0816 16:50:59.845170   91938 provision.go:138] copyHostCerts
I0816 16:50:59.846947   91938 exec_runner.go:144] found /Users/kuldeep/.minikube/cert.pem, removing ...
I0816 16:50:59.846991   91938 exec_runner.go:203] rm: /Users/kuldeep/.minikube/cert.pem
I0816 16:50:59.847350   91938 exec_runner.go:151] cp: /Users/kuldeep/.minikube/certs/cert.pem --> /Users/kuldeep/.minikube/cert.pem (1123 bytes)
I0816 16:50:59.847753   91938 exec_runner.go:144] found /Users/kuldeep/.minikube/key.pem, removing ...
I0816 16:50:59.847756   91938 exec_runner.go:203] rm: /Users/kuldeep/.minikube/key.pem
I0816 16:50:59.847806   91938 exec_runner.go:151] cp: /Users/kuldeep/.minikube/certs/key.pem --> /Users/kuldeep/.minikube/key.pem (1675 bytes)
I0816 16:50:59.848057   91938 exec_runner.go:144] found /Users/kuldeep/.minikube/ca.pem, removing ...
I0816 16:50:59.848059   91938 exec_runner.go:203] rm: /Users/kuldeep/.minikube/ca.pem
I0816 16:50:59.848107   91938 exec_runner.go:151] cp: /Users/kuldeep/.minikube/certs/ca.pem --> /Users/kuldeep/.minikube/ca.pem (1082 bytes)
I0816 16:50:59.848212   91938 provision.go:112] generating server cert: /Users/kuldeep/.minikube/machines/server.pem ca-key=/Users/kuldeep/.minikube/certs/ca.pem private-key=/Users/kuldeep/.minikube/certs/ca-key.pem org=kuldeep.minikube san=[192.168.105.2 192.168.105.2 localhost 127.0.0.1 minikube minikube]
I0816 16:50:59.903145   91938 provision.go:172] copyRemoteCerts
I0816 16:50:59.904345   91938 ssh_runner.go:195] Run: sudo mkdir -p /etc/docker /etc/docker /etc/docker
I0816 16:50:59.904473   91938 sshutil.go:53] new ssh client: &{IP:192.168.105.2 Port:22 SSHKeyPath:/Users/kuldeep/.minikube/machines/minikube/id_rsa Username:docker}
I0816 16:50:59.946229   91938 ssh_runner.go:362] scp /Users/kuldeep/.minikube/certs/ca.pem --> /etc/docker/ca.pem (1082 bytes)
I0816 16:50:59.959633   91938 ssh_runner.go:362] scp /Users/kuldeep/.minikube/machines/server.pem --> /etc/docker/server.pem (1204 bytes)
I0816 16:50:59.969605   91938 ssh_runner.go:362] scp /Users/kuldeep/.minikube/machines/server-key.pem --> /etc/docker/server-key.pem (1675 bytes)
I0816 16:50:59.983284   91938 provision.go:86] duration metric: configureAuth took 138.110375ms
I0816 16:50:59.983498   91938 buildroot.go:189] setting minikube options for container-runtime
I0816 16:50:59.984705   91938 config.go:182] Loaded profile config "minikube": Driver=qemu2, ContainerRuntime=docker, KubernetesVersion=v1.27.3
I0816 16:50:59.985180   91938 main.go:141] libmachine: Using SSH client type: native
I0816 16:50:59.985431   91938 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x10451b4b0] 0x10451df10 <nil>  [] 0s} 192.168.105.2 22 <nil> <nil>}
I0816 16:50:59.985435   91938 main.go:141] libmachine: About to run SSH command:
df --output=fstype / | tail -n 1
I0816 16:51:00.062220   91938 main.go:141] libmachine: SSH cmd err, output: <nil>: tmpfs

I0816 16:51:00.062227   91938 buildroot.go:70] root file system type: tmpfs
I0816 16:51:00.064035   91938 provision.go:309] Updating docker unit: /lib/systemd/system/docker.service ...
I0816 16:51:00.064282   91938 main.go:141] libmachine: Using SSH client type: native
I0816 16:51:00.064523   91938 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x10451b4b0] 0x10451df10 <nil>  [] 0s} 192.168.105.2 22 <nil> <nil>}
I0816 16:51:00.064555   91938 main.go:141] libmachine: About to run SSH command:
sudo mkdir -p /lib/systemd/system && printf %!s(MISSING) "[Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
After=network.target  minikube-automount.service docker.socket
Requires= minikube-automount.service docker.socket 
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=qemu2 --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP \$MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target
" | sudo tee /lib/systemd/system/docker.service.new
I0816 16:51:00.144626   91938 main.go:141] libmachine: SSH cmd err, output: <nil>: [Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
After=network.target  minikube-automount.service docker.socket
Requires= minikube-automount.service docker.socket 
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=qemu2 --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP $MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target

I0816 16:51:00.144958   91938 main.go:141] libmachine: Using SSH client type: native
I0816 16:51:00.145203   91938 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x10451b4b0] 0x10451df10 <nil>  [] 0s} 192.168.105.2 22 <nil> <nil>}
I0816 16:51:00.145210   91938 main.go:141] libmachine: About to run SSH command:
sudo diff -u /lib/systemd/system/docker.service /lib/systemd/system/docker.service.new || { sudo mv /lib/systemd/system/docker.service.new /lib/systemd/system/docker.service; sudo systemctl -f daemon-reload && sudo systemctl -f enable docker && sudo systemctl -f restart docker; }
I0816 16:51:00.913484   91938 main.go:141] libmachine: SSH cmd err, output: <nil>: diff: can't stat '/lib/systemd/system/docker.service': No such file or directory
Created symlink /etc/systemd/system/multi-user.target.wants/docker.service ‚Üí /usr/lib/systemd/system/docker.service.

I0816 16:51:00.913498   91938 machine.go:91] provisioned docker machine in 1.260795167s
I0816 16:51:00.913764   91938 start.go:300] post-start starting for "minikube" (driver="qemu2")
I0816 16:51:00.914015   91938 start.go:329] creating required directories: [/etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs]
I0816 16:51:00.914252   91938 ssh_runner.go:195] Run: sudo mkdir -p /etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs
I0816 16:51:00.914261   91938 sshutil.go:53] new ssh client: &{IP:192.168.105.2 Port:22 SSHKeyPath:/Users/kuldeep/.minikube/machines/minikube/id_rsa Username:docker}
I0816 16:51:00.957079   91938 ssh_runner.go:195] Run: cat /etc/os-release
I0816 16:51:00.959147   91938 info.go:137] Remote host: Buildroot 2021.02.12
I0816 16:51:00.959312   91938 filesync.go:126] Scanning /Users/kuldeep/.minikube/addons for local assets ...
I0816 16:51:00.959717   91938 filesync.go:126] Scanning /Users/kuldeep/.minikube/files for local assets ...
I0816 16:51:00.959744   91938 start.go:303] post-start completed in 45.839709ms
I0816 16:51:00.959747   91938 fix.go:56] fixHost completed within 36.527615083s
I0816 16:51:00.959950   91938 main.go:141] libmachine: Using SSH client type: native
I0816 16:51:00.960185   91938 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x10451b4b0] 0x10451df10 <nil>  [] 0s} 192.168.105.2 22 <nil> <nil>}
I0816 16:51:00.960188   91938 main.go:141] libmachine: About to run SSH command:
date +%!s(MISSING).%!N(MISSING)
I0816 16:51:01.037840   91938 main.go:141] libmachine: SSH cmd err, output: <nil>: 1692201061.060773013

I0816 16:51:01.037851   91938 fix.go:206] guest clock: 1692201061.060773013
I0816 16:51:01.037854   91938 fix.go:219] Guest: 2023-08-16 16:51:01.060773013 +0100 BST Remote: 2023-08-16 16:51:00.95975 +0100 BST m=+36.736158584 (delta=101.023013ms)
I0816 16:51:01.037866   91938 fix.go:190] guest clock delta is within tolerance: 101.023013ms
I0816 16:51:01.038078   91938 start.go:83] releasing machines lock for "minikube", held for 36.605960208s
I0816 16:51:01.039767   91938 ssh_runner.go:195] Run: curl -sS -m 2 https://registry.k8s.io/
I0816 16:51:01.040066   91938 ssh_runner.go:195] Run: cat /version.json
I0816 16:51:01.040075   91938 sshutil.go:53] new ssh client: &{IP:192.168.105.2 Port:22 SSHKeyPath:/Users/kuldeep/.minikube/machines/minikube/id_rsa Username:docker}
I0816 16:51:01.040534   91938 sshutil.go:53] new ssh client: &{IP:192.168.105.2 Port:22 SSHKeyPath:/Users/kuldeep/.minikube/machines/minikube/id_rsa Username:docker}
I0816 16:51:01.080595   91938 ssh_runner.go:195] Run: systemctl --version
I0816 16:51:01.187487   91938 ssh_runner.go:195] Run: sh -c "stat /etc/cni/net.d/*loopback.conf*"
W0816 16:51:01.190539   91938 cni.go:209] loopback cni configuration skipped: "/etc/cni/net.d/*loopback.conf*" not found
I0816 16:51:01.190715   91938 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f ( ( -name *bridge* -or -name *podman* ) -and -not -name *.mk_disabled ) -printf "%!p(MISSING), " -exec sh -c "sudo mv {} {}.mk_disabled" ;
I0816 16:51:01.196059   91938 cni.go:262] disabled [/etc/cni/net.d/87-podman-bridge.conflist] bridge cni config(s)
I0816 16:51:01.196409   91938 start.go:466] detecting cgroup driver to use...
I0816 16:51:01.197561   91938 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %!s(MISSING) "runtime-endpoint: unix:///run/containerd/containerd.sock
" | sudo tee /etc/crictl.yaml"
I0816 16:51:01.204263   91938 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)sandbox_image = .*$|\1sandbox_image = "registry.k8s.io/pause:3.9"|' /etc/containerd/config.toml"
I0816 16:51:01.207599   91938 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)restrict_oom_score_adj = .*$|\1restrict_oom_score_adj = false|' /etc/containerd/config.toml"
I0816 16:51:01.211266   91938 containerd.go:145] configuring containerd to use "cgroupfs" as cgroup driver...
I0816 16:51:01.211344   91938 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)SystemdCgroup = .*$|\1SystemdCgroup = false|g' /etc/containerd/config.toml"
I0816 16:51:01.214590   91938 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runtime.v1.linux"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I0816 16:51:01.217910   91938 ssh_runner.go:195] Run: sh -c "sudo sed -i '/systemd_cgroup/d' /etc/containerd/config.toml"
I0816 16:51:01.221765   91938 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runc.v1"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I0816 16:51:01.225019   91938 ssh_runner.go:195] Run: sh -c "sudo rm -rf /etc/cni/net.mk"
I0816 16:51:01.228537   91938 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)conf_dir = .*$|\1conf_dir = "/etc/cni/net.d"|g' /etc/containerd/config.toml"
I0816 16:51:01.231724   91938 ssh_runner.go:195] Run: sudo sysctl net.bridge.bridge-nf-call-iptables
I0816 16:51:01.234629   91938 ssh_runner.go:195] Run: sudo sh -c "echo 1 > /proc/sys/net/ipv4/ip_forward"
I0816 16:51:01.237927   91938 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0816 16:51:01.321890   91938 ssh_runner.go:195] Run: sudo systemctl restart containerd
I0816 16:51:01.331684   91938 start.go:466] detecting cgroup driver to use...
I0816 16:51:01.332628   91938 ssh_runner.go:195] Run: sudo systemctl cat docker.service
I0816 16:51:01.339969   91938 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service containerd
I0816 16:51:01.345842   91938 ssh_runner.go:195] Run: sudo systemctl stop -f containerd
I0816 16:51:01.358164   91938 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service containerd
I0816 16:51:01.363533   91938 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service crio
I0816 16:51:01.368121   91938 ssh_runner.go:195] Run: sudo systemctl stop -f crio
I0816 16:51:01.417331   91938 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service crio
I0816 16:51:01.423216   91938 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %!s(MISSING) "runtime-endpoint: unix:///var/run/cri-dockerd.sock
" | sudo tee /etc/crictl.yaml"
I0816 16:51:01.429891   91938 ssh_runner.go:195] Run: which cri-dockerd
I0816 16:51:01.431816   91938 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/cri-docker.service.d
I0816 16:51:01.434688   91938 ssh_runner.go:362] scp memory --> /etc/systemd/system/cri-docker.service.d/10-cni.conf (189 bytes)
I0816 16:51:01.441857   91938 ssh_runner.go:195] Run: sudo systemctl unmask docker.service
I0816 16:51:01.522343   91938 ssh_runner.go:195] Run: sudo systemctl enable docker.socket
I0816 16:51:01.608112   91938 docker.go:535] configuring docker to use "cgroupfs" as cgroup driver...
I0816 16:51:01.608136   91938 ssh_runner.go:362] scp memory --> /etc/docker/daemon.json (144 bytes)
I0816 16:51:01.614131   91938 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0816 16:51:01.697540   91938 ssh_runner.go:195] Run: sudo systemctl restart docker
I0816 16:51:02.917377   91938 ssh_runner.go:235] Completed: sudo systemctl restart docker: (1.219812875s)
I0816 16:51:02.917518   91938 ssh_runner.go:195] Run: sudo systemctl enable cri-docker.socket
I0816 16:51:02.989072   91938 ssh_runner.go:195] Run: sudo systemctl unmask cri-docker.socket
I0816 16:51:03.074495   91938 ssh_runner.go:195] Run: sudo systemctl enable cri-docker.socket
I0816 16:51:03.154337   91938 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0816 16:51:03.238698   91938 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.socket
I0816 16:51:03.245268   91938 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0816 16:51:03.323921   91938 ssh_runner.go:195] Run: sudo systemctl restart cri-docker
I0816 16:51:03.354026   91938 start.go:513] Will wait 60s for socket path /var/run/cri-dockerd.sock
I0816 16:51:03.354464   91938 ssh_runner.go:195] Run: stat /var/run/cri-dockerd.sock
I0816 16:51:03.356679   91938 start.go:534] Will wait 60s for crictl version
I0816 16:51:03.356777   91938 ssh_runner.go:195] Run: which crictl
I0816 16:51:03.358124   91938 ssh_runner.go:195] Run: sudo /usr/bin/crictl version
I0816 16:51:03.378269   91938 start.go:550] Version:  0.1.0
RuntimeName:  docker
RuntimeVersion:  24.0.4
RuntimeApiVersion:  v1alpha2
I0816 16:51:03.378398   91938 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0816 16:51:03.391870   91938 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0816 16:51:03.406700   91938 out.go:204] üê≥  Preparing Kubernetes v1.27.3 on Docker 24.0.4 ...
I0816 16:51:03.407288   91938 ssh_runner.go:195] Run: grep 192.168.105.1	host.minikube.internal$ /etc/hosts
I0816 16:51:03.408894   91938 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\thost.minikube.internal$' "/etc/hosts"; echo "192.168.105.1	host.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I0816 16:51:03.413912   91938 preload.go:132] Checking if preload exists for k8s version v1.27.3 and runtime docker
I0816 16:51:03.414109   91938 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I0816 16:51:03.424172   91938 docker.go:636] Got preloaded images: -- stdout --
kdmalviyan/k8s-app-spring:latest
registry.k8s.io/kube-apiserver:v1.27.3
registry.k8s.io/kube-controller-manager:v1.27.3
registry.k8s.io/kube-scheduler:v1.27.3
registry.k8s.io/kube-proxy:v1.27.3
registry.k8s.io/metrics-server/metrics-server:<none>
registry.k8s.io/coredns/coredns:v1.10.1
registry.k8s.io/etcd:3.5.7-0
registry.k8s.io/pause:3.9
kubernetesui/dashboard:<none>
kubernetesui/metrics-scraper:<none>
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I0816 16:51:03.424363   91938 docker.go:566] Images already preloaded, skipping extraction
I0816 16:51:03.424601   91938 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I0816 16:51:03.430564   91938 docker.go:636] Got preloaded images: -- stdout --
kdmalviyan/k8s-app-spring:latest
registry.k8s.io/kube-apiserver:v1.27.3
registry.k8s.io/kube-scheduler:v1.27.3
registry.k8s.io/kube-proxy:v1.27.3
registry.k8s.io/kube-controller-manager:v1.27.3
registry.k8s.io/metrics-server/metrics-server:<none>
registry.k8s.io/coredns/coredns:v1.10.1
registry.k8s.io/etcd:3.5.7-0
registry.k8s.io/pause:3.9
kubernetesui/dashboard:<none>
kubernetesui/metrics-scraper:<none>
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I0816 16:51:03.430570   91938 cache_images.go:84] Images are preloaded, skipping loading
I0816 16:51:03.430886   91938 ssh_runner.go:195] Run: docker info --format {{.CgroupDriver}}
I0816 16:51:03.438981   91938 cni.go:84] Creating CNI manager for ""
I0816 16:51:03.439118   91938 cni.go:158] "qemu2" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0816 16:51:03.439953   91938 kubeadm.go:87] Using pod CIDR: 10.244.0.0/16
I0816 16:51:03.439963   91938 kubeadm.go:176] kubeadm options: {CertDir:/var/lib/minikube/certs ServiceCIDR:10.96.0.0/12 PodSubnet:10.244.0.0/16 AdvertiseAddress:192.168.105.2 APIServerPort:8443 KubernetesVersion:v1.27.3 EtcdDataDir:/var/lib/minikube/etcd EtcdExtraArgs:map[] ClusterName:minikube NodeName:minikube DNSDomain:cluster.local CRISocket:/var/run/cri-dockerd.sock ImageRepository: ComponentOptions:[{Component:apiServer ExtraArgs:map[enable-admission-plugins:NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota] Pairs:map[certSANs:["127.0.0.1", "localhost", "192.168.105.2"]]} {Component:controllerManager ExtraArgs:map[allocate-node-cidrs:true leader-elect:false] Pairs:map[]} {Component:scheduler ExtraArgs:map[leader-elect:false] Pairs:map[]}] FeatureArgs:map[] NodeIP:192.168.105.2 CgroupDriver:cgroupfs ClientCAFile:/var/lib/minikube/certs/ca.crt StaticPodPath:/etc/kubernetes/manifests ControlPlaneAddress:control-plane.minikube.internal KubeProxyOptions:map[] ResolvConfSearchRegression:false KubeletConfigOpts:map[hairpinMode:hairpin-veth runtimeRequestTimeout:15m] PrependCriSocketUnix:true}
I0816 16:51:03.440550   91938 kubeadm.go:181] kubeadm config:
apiVersion: kubeadm.k8s.io/v1beta3
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 192.168.105.2
  bindPort: 8443
bootstrapTokens:
  - groups:
      - system:bootstrappers:kubeadm:default-node-token
    ttl: 24h0m0s
    usages:
      - signing
      - authentication
nodeRegistration:
  criSocket: unix:///var/run/cri-dockerd.sock
  name: "minikube"
  kubeletExtraArgs:
    node-ip: 192.168.105.2
  taints: []
---
apiVersion: kubeadm.k8s.io/v1beta3
kind: ClusterConfiguration
apiServer:
  certSANs: ["127.0.0.1", "localhost", "192.168.105.2"]
  extraArgs:
    enable-admission-plugins: "NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota"
controllerManager:
  extraArgs:
    allocate-node-cidrs: "true"
    leader-elect: "false"
scheduler:
  extraArgs:
    leader-elect: "false"
certificatesDir: /var/lib/minikube/certs
clusterName: mk
controlPlaneEndpoint: control-plane.minikube.internal:8443
etcd:
  local:
    dataDir: /var/lib/minikube/etcd
    extraArgs:
      proxy-refresh-interval: "70000"
kubernetesVersion: v1.27.3
networking:
  dnsDomain: cluster.local
  podSubnet: "10.244.0.0/16"
  serviceSubnet: 10.96.0.0/12
---
apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
authentication:
  x509:
    clientCAFile: /var/lib/minikube/certs/ca.crt
cgroupDriver: cgroupfs
hairpinMode: hairpin-veth
runtimeRequestTimeout: 15m
clusterDomain: "cluster.local"
# disable disk resource management by default
imageGCHighThresholdPercent: 100
evictionHard:
  nodefs.available: "0%!"(MISSING)
  nodefs.inodesFree: "0%!"(MISSING)
  imagefs.available: "0%!"(MISSING)
failSwapOn: false
staticPodPath: /etc/kubernetes/manifests
---
apiVersion: kubeproxy.config.k8s.io/v1alpha1
kind: KubeProxyConfiguration
clusterCIDR: "10.244.0.0/16"
metricsBindAddress: 0.0.0.0:10249
conntrack:
  maxPerCore: 0
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_established"
  tcpEstablishedTimeout: 0s
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_close"
  tcpCloseWaitTimeout: 0s

I0816 16:51:03.441124   91938 kubeadm.go:976] kubelet [Unit]
Wants=docker.socket

[Service]
ExecStart=
ExecStart=/var/lib/minikube/binaries/v1.27.3/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --config=/var/lib/kubelet/config.yaml --container-runtime-endpoint=unix:///var/run/cri-dockerd.sock --hostname-override=minikube --kubeconfig=/etc/kubernetes/kubelet.conf --node-ip=192.168.105.2

[Install]
 config:
{KubernetesVersion:v1.27.3 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:}
I0816 16:51:03.441252   91938 ssh_runner.go:195] Run: sudo ls /var/lib/minikube/binaries/v1.27.3
I0816 16:51:03.445113   91938 binaries.go:44] Found k8s binaries, skipping transfer
I0816 16:51:03.445418   91938 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/kubelet.service.d /lib/systemd/system /var/tmp/minikube
I0816 16:51:03.448570   91938 ssh_runner.go:362] scp memory --> /etc/systemd/system/kubelet.service.d/10-kubeadm.conf (370 bytes)
I0816 16:51:03.453710   91938 ssh_runner.go:362] scp memory --> /lib/systemd/system/kubelet.service (352 bytes)
I0816 16:51:03.458548   91938 ssh_runner.go:362] scp memory --> /var/tmp/minikube/kubeadm.yaml.new (2094 bytes)
I0816 16:51:03.464089   91938 ssh_runner.go:195] Run: grep 192.168.105.2	control-plane.minikube.internal$ /etc/hosts
I0816 16:51:03.465393   91938 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\tcontrol-plane.minikube.internal$' "/etc/hosts"; echo "192.168.105.2	control-plane.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I0816 16:51:03.469702   91938 certs.go:56] Setting up /Users/kuldeep/.minikube/profiles/minikube for IP: 192.168.105.2
I0816 16:51:03.469849   91938 certs.go:190] acquiring lock for shared ca certs: {Name:mkb16f2f1c9954db60e8fdd45095a731100f212b Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0816 16:51:03.471293   91938 certs.go:199] skipping minikubeCA CA generation: /Users/kuldeep/.minikube/ca.key
I0816 16:51:03.471465   91938 certs.go:199] skipping proxyClientCA CA generation: /Users/kuldeep/.minikube/proxy-client-ca.key
I0816 16:51:03.471784   91938 certs.go:315] skipping minikube-user signed cert generation: /Users/kuldeep/.minikube/profiles/minikube/client.key
I0816 16:51:03.471992   91938 certs.go:315] skipping minikube signed cert generation: /Users/kuldeep/.minikube/profiles/minikube/apiserver.key.96055969
I0816 16:51:03.472114   91938 certs.go:315] skipping aggregator signed cert generation: /Users/kuldeep/.minikube/profiles/minikube/proxy-client.key
I0816 16:51:03.472265   91938 certs.go:437] found cert: /Users/kuldeep/.minikube/certs/Users/kuldeep/.minikube/certs/ca-key.pem (1679 bytes)
I0816 16:51:03.472297   91938 certs.go:437] found cert: /Users/kuldeep/.minikube/certs/Users/kuldeep/.minikube/certs/ca.pem (1082 bytes)
I0816 16:51:03.472319   91938 certs.go:437] found cert: /Users/kuldeep/.minikube/certs/Users/kuldeep/.minikube/certs/cert.pem (1123 bytes)
I0816 16:51:03.472339   91938 certs.go:437] found cert: /Users/kuldeep/.minikube/certs/Users/kuldeep/.minikube/certs/key.pem (1675 bytes)
I0816 16:51:03.477463   91938 ssh_runner.go:362] scp /Users/kuldeep/.minikube/profiles/minikube/apiserver.crt --> /var/lib/minikube/certs/apiserver.crt (1399 bytes)
I0816 16:51:03.487926   91938 ssh_runner.go:362] scp /Users/kuldeep/.minikube/profiles/minikube/apiserver.key --> /var/lib/minikube/certs/apiserver.key (1679 bytes)
I0816 16:51:03.494939   91938 ssh_runner.go:362] scp /Users/kuldeep/.minikube/profiles/minikube/proxy-client.crt --> /var/lib/minikube/certs/proxy-client.crt (1147 bytes)
I0816 16:51:03.502046   91938 ssh_runner.go:362] scp /Users/kuldeep/.minikube/profiles/minikube/proxy-client.key --> /var/lib/minikube/certs/proxy-client.key (1679 bytes)
I0816 16:51:03.509280   91938 ssh_runner.go:362] scp /Users/kuldeep/.minikube/ca.crt --> /var/lib/minikube/certs/ca.crt (1111 bytes)
I0816 16:51:03.516385   91938 ssh_runner.go:362] scp /Users/kuldeep/.minikube/ca.key --> /var/lib/minikube/certs/ca.key (1679 bytes)
I0816 16:51:03.523570   91938 ssh_runner.go:362] scp /Users/kuldeep/.minikube/proxy-client-ca.crt --> /var/lib/minikube/certs/proxy-client-ca.crt (1119 bytes)
I0816 16:51:03.530303   91938 ssh_runner.go:362] scp /Users/kuldeep/.minikube/proxy-client-ca.key --> /var/lib/minikube/certs/proxy-client-ca.key (1679 bytes)
I0816 16:51:03.537343   91938 ssh_runner.go:362] scp /Users/kuldeep/.minikube/ca.crt --> /usr/share/ca-certificates/minikubeCA.pem (1111 bytes)
I0816 16:51:03.544429   91938 ssh_runner.go:362] scp memory --> /var/lib/minikube/kubeconfig (738 bytes)
I0816 16:51:03.552160   91938 ssh_runner.go:195] Run: openssl version
I0816 16:51:03.554596   91938 ssh_runner.go:195] Run: sudo /bin/bash -c "test -s /usr/share/ca-certificates/minikubeCA.pem && ln -fs /usr/share/ca-certificates/minikubeCA.pem /etc/ssl/certs/minikubeCA.pem"
I0816 16:51:03.558234   91938 ssh_runner.go:195] Run: ls -la /usr/share/ca-certificates/minikubeCA.pem
I0816 16:51:03.559881   91938 certs.go:480] hashing: -rw-r--r-- 1 root root 1111 Aug  6 08:07 /usr/share/ca-certificates/minikubeCA.pem
I0816 16:51:03.559945   91938 ssh_runner.go:195] Run: openssl x509 -hash -noout -in /usr/share/ca-certificates/minikubeCA.pem
I0816 16:51:03.561853   91938 ssh_runner.go:195] Run: sudo /bin/bash -c "test -L /etc/ssl/certs/b5213941.0 || ln -fs /etc/ssl/certs/minikubeCA.pem /etc/ssl/certs/b5213941.0"
I0816 16:51:03.564938   91938 ssh_runner.go:195] Run: ls /var/lib/minikube/certs/etcd
I0816 16:51:03.566583   91938 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/apiserver-etcd-client.crt -checkend 86400
I0816 16:51:03.568796   91938 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/apiserver-kubelet-client.crt -checkend 86400
I0816 16:51:03.570827   91938 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/server.crt -checkend 86400
I0816 16:51:03.572753   91938 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/healthcheck-client.crt -checkend 86400
I0816 16:51:03.574690   91938 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/peer.crt -checkend 86400
I0816 16:51:03.576651   91938 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/front-proxy-client.crt -checkend 86400
I0816 16:51:03.578731   91938 kubeadm.go:404] StartCluster: {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO:https://storage.googleapis.com/minikube/iso/minikube-v1.31.0-arm64.iso KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.40@sha256:8cadf23777709e43eca447c47a45f5a4635615129267ce025193040ec92a1631 Memory:2200 CPUs:2 DiskSize:20000 VMDriver: Driver:qemu2 HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:0 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.27.3 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:} Nodes:[{Name: IP:192.168.105.2 Port:8443 KubernetesVersion:v1.27.3 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[dashboard:true default-storageclass:true metrics-server:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network:socket_vmnet Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/Users:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath:/opt/homebrew/opt/socket_vmnet/bin/socket_vmnet_client SocketVMnetPath:/opt/homebrew/var/run/socket_vmnet StaticIP: SSHAuthSock: SSHAgentPID:0}
I0816 16:51:03.578864   91938 ssh_runner.go:195] Run: docker ps --filter status=paused --filter=name=k8s_.*_(kube-system)_ --format={{.ID}}
I0816 16:51:03.587979   91938 ssh_runner.go:195] Run: sudo ls /var/lib/kubelet/kubeadm-flags.env /var/lib/kubelet/config.yaml /var/lib/minikube/etcd
I0816 16:51:03.591437   91938 kubeadm.go:419] found existing configuration files, will attempt cluster restart
I0816 16:51:03.591457   91938 kubeadm.go:636] restartCluster start
I0816 16:51:03.591516   91938 ssh_runner.go:195] Run: sudo test -d /data/minikube
I0816 16:51:03.594410   91938 kubeadm.go:127] /data/minikube skipping compat symlinks: sudo test -d /data/minikube: Process exited with status 1
stdout:

stderr:
I0816 16:51:03.596751   91938 kubeconfig.go:92] found "minikube" server: "https://192.168.105.2:8443"
I0816 16:51:03.605605   91938 ssh_runner.go:195] Run: sudo diff -u /var/tmp/minikube/kubeadm.yaml /var/tmp/minikube/kubeadm.yaml.new
I0816 16:51:03.614527   91938 api_server.go:166] Checking apiserver status ...
I0816 16:51:03.614598   91938 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0816 16:51:03.619001   91938 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0816 16:51:04.119908   91938 api_server.go:166] Checking apiserver status ...
I0816 16:51:04.120598   91938 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0816 16:51:04.134188   91938 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0816 16:51:04.619252   91938 api_server.go:166] Checking apiserver status ...
I0816 16:51:04.619565   91938 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0816 16:51:04.632445   91938 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0816 16:51:05.120132   91938 api_server.go:166] Checking apiserver status ...
I0816 16:51:05.120560   91938 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0816 16:51:05.133834   91938 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0816 16:51:05.620113   91938 api_server.go:166] Checking apiserver status ...
I0816 16:51:05.620561   91938 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0816 16:51:05.634221   91938 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0816 16:51:06.120094   91938 api_server.go:166] Checking apiserver status ...
I0816 16:51:06.120325   91938 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0816 16:51:06.129698   91938 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0816 16:51:06.620146   91938 api_server.go:166] Checking apiserver status ...
I0816 16:51:06.620443   91938 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0816 16:51:06.629640   91938 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0816 16:51:07.120106   91938 api_server.go:166] Checking apiserver status ...
I0816 16:51:07.120320   91938 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0816 16:51:07.132762   91938 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0816 16:51:07.620118   91938 api_server.go:166] Checking apiserver status ...
I0816 16:51:07.620344   91938 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0816 16:51:07.629545   91938 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0816 16:51:08.119560   91938 api_server.go:166] Checking apiserver status ...
I0816 16:51:08.119968   91938 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0816 16:51:08.131103   91938 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0816 16:51:08.620153   91938 api_server.go:166] Checking apiserver status ...
I0816 16:51:08.620500   91938 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0816 16:51:08.630709   91938 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0816 16:51:09.120163   91938 api_server.go:166] Checking apiserver status ...
I0816 16:51:09.120934   91938 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0816 16:51:09.131416   91938 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0816 16:51:09.620168   91938 api_server.go:166] Checking apiserver status ...
I0816 16:51:09.620436   91938 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0816 16:51:09.630030   91938 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0816 16:51:10.120143   91938 api_server.go:166] Checking apiserver status ...
I0816 16:51:10.120374   91938 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0816 16:51:10.129841   91938 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0816 16:51:10.620179   91938 api_server.go:166] Checking apiserver status ...
I0816 16:51:10.620589   91938 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0816 16:51:10.630551   91938 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0816 16:51:11.120201   91938 api_server.go:166] Checking apiserver status ...
I0816 16:51:11.120584   91938 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0816 16:51:11.129208   91938 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0816 16:51:11.620177   91938 api_server.go:166] Checking apiserver status ...
I0816 16:51:11.620633   91938 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0816 16:51:11.633326   91938 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0816 16:51:12.120156   91938 api_server.go:166] Checking apiserver status ...
I0816 16:51:12.120408   91938 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0816 16:51:12.130362   91938 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0816 16:51:12.620232   91938 api_server.go:166] Checking apiserver status ...
I0816 16:51:12.622029   91938 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0816 16:51:12.636893   91938 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0816 16:51:13.120239   91938 api_server.go:166] Checking apiserver status ...
I0816 16:51:13.120830   91938 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0816 16:51:13.135999   91938 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0816 16:51:13.615922   91938 kubeadm.go:611] needs reconfigure: apiserver error: context deadline exceeded
I0816 16:51:13.615959   91938 kubeadm.go:1128] stopping kube-system containers ...
I0816 16:51:13.616282   91938 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_.*_(kube-system)_ --format={{.ID}}
I0816 16:51:13.657243   91938 docker.go:462] Stopping containers: [ffd9479bde51 e25e7bb53be2 fb28c45afa76 63855b84420c 6da0cdf7457c 13505e867f2d b0f5f807e4cf 3de0811c09a8 cd09005e9e06 5a9deb8a0f84 32d50646391c 1f3795d91eeb 31be563eb51c 00f4858c4115 cba4066607c8 7ffb4df97592 dfa3cdd416b2 cd966556267a 071ba751edca 608a2c2dfca7 fbbc407bc612 3c91b912eb0a 0c90eac9ea3e c42abb4b6867 55664d31dcfc a15ff176fea3 a6c23d353a8c 31f7330dc8f3 97ec59625c6a 27fa4f54cba6 c55b02d65a63]
I0816 16:51:13.657428   91938 ssh_runner.go:195] Run: docker stop ffd9479bde51 e25e7bb53be2 fb28c45afa76 63855b84420c 6da0cdf7457c 13505e867f2d b0f5f807e4cf 3de0811c09a8 cd09005e9e06 5a9deb8a0f84 32d50646391c 1f3795d91eeb 31be563eb51c 00f4858c4115 cba4066607c8 7ffb4df97592 dfa3cdd416b2 cd966556267a 071ba751edca 608a2c2dfca7 fbbc407bc612 3c91b912eb0a 0c90eac9ea3e c42abb4b6867 55664d31dcfc a15ff176fea3 a6c23d353a8c 31f7330dc8f3 97ec59625c6a 27fa4f54cba6 c55b02d65a63
I0816 16:51:13.671973   91938 ssh_runner.go:195] Run: sudo systemctl stop kubelet
I0816 16:51:13.684344   91938 ssh_runner.go:195] Run: sudo ls -la /etc/kubernetes/admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf
I0816 16:51:13.689247   91938 kubeadm.go:152] config check failed, skipping stale config cleanup: sudo ls -la /etc/kubernetes/admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf: Process exited with status 2
stdout:

stderr:
ls: cannot access '/etc/kubernetes/admin.conf': No such file or directory
ls: cannot access '/etc/kubernetes/kubelet.conf': No such file or directory
ls: cannot access '/etc/kubernetes/controller-manager.conf': No such file or directory
ls: cannot access '/etc/kubernetes/scheduler.conf': No such file or directory
I0816 16:51:13.689347   91938 ssh_runner.go:195] Run: sudo cp /var/tmp/minikube/kubeadm.yaml.new /var/tmp/minikube/kubeadm.yaml
I0816 16:51:13.693720   91938 kubeadm.go:713] reconfiguring cluster from /var/tmp/minikube/kubeadm.yaml
I0816 16:51:13.693724   91938 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.27.3:$PATH" kubeadm init phase certs all --config /var/tmp/minikube/kubeadm.yaml"
I0816 16:51:13.786884   91938 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.27.3:$PATH" kubeadm init phase kubeconfig all --config /var/tmp/minikube/kubeadm.yaml"
I0816 16:51:14.325329   91938 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.27.3:$PATH" kubeadm init phase kubelet-start --config /var/tmp/minikube/kubeadm.yaml"
I0816 16:51:14.433710   91938 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.27.3:$PATH" kubeadm init phase control-plane all --config /var/tmp/minikube/kubeadm.yaml"
I0816 16:51:14.457739   91938 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.27.3:$PATH" kubeadm init phase etcd local --config /var/tmp/minikube/kubeadm.yaml"
I0816 16:51:14.481047   91938 api_server.go:52] waiting for apiserver process to appear ...
I0816 16:51:14.481188   91938 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0816 16:51:14.988945   91938 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0816 16:51:15.486765   91938 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0816 16:51:15.986731   91938 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0816 16:51:16.487065   91938 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0816 16:51:16.493975   91938 api_server.go:72] duration metric: took 2.012905709s to wait for apiserver process to appear ...
I0816 16:51:16.493992   91938 api_server.go:88] waiting for apiserver healthz status ...
I0816 16:51:16.494002   91938 api_server.go:253] Checking apiserver healthz at https://192.168.105.2:8443/healthz ...
I0816 16:51:18.874413   91938 api_server.go:279] https://192.168.105.2:8443/healthz returned 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
W0816 16:51:18.874471   91938 api_server.go:103] status: https://192.168.105.2:8443/healthz returned error 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
I0816 16:51:19.374589   91938 api_server.go:253] Checking apiserver healthz at https://192.168.105.2:8443/healthz ...
I0816 16:51:19.405441   91938 api_server.go:279] https://192.168.105.2:8443/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
W0816 16:51:19.405471   91938 api_server.go:103] status: https://192.168.105.2:8443/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
I0816 16:51:19.875162   91938 api_server.go:253] Checking apiserver healthz at https://192.168.105.2:8443/healthz ...
I0816 16:51:19.879194   91938 api_server.go:279] https://192.168.105.2:8443/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
W0816 16:51:19.879203   91938 api_server.go:103] status: https://192.168.105.2:8443/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
I0816 16:51:20.374608   91938 api_server.go:253] Checking apiserver healthz at https://192.168.105.2:8443/healthz ...
I0816 16:51:20.379068   91938 api_server.go:279] https://192.168.105.2:8443/healthz returned 200:
ok
I0816 16:51:20.391886   91938 api_server.go:141] control plane version: v1.27.3
I0816 16:51:20.392021   91938 api_server.go:131] duration metric: took 3.897985875s to wait for apiserver health ...
I0816 16:51:20.393065   91938 cni.go:84] Creating CNI manager for ""
I0816 16:51:20.393272   91938 cni.go:158] "qemu2" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0816 16:51:20.401677   91938 out.go:177] üîó  Configuring bridge CNI (Container Networking Interface) ...
I0816 16:51:20.405063   91938 ssh_runner.go:195] Run: sudo mkdir -p /etc/cni/net.d
I0816 16:51:20.410448   91938 ssh_runner.go:362] scp memory --> /etc/cni/net.d/1-k8s.conflist (457 bytes)
I0816 16:51:20.421900   91938 system_pods.go:43] waiting for kube-system pods to appear ...
I0816 16:51:20.438728   91938 system_pods.go:59] 8 kube-system pods found
I0816 16:51:20.438742   91938 system_pods.go:61] "coredns-5d78c9869d-ptqjm" [05e0c7e4-0c61-4fa0-a77c-ce1f35365062] Running / Ready:ContainersNotReady (containers with unready status: [coredns]) / ContainersReady:ContainersNotReady (containers with unready status: [coredns])
I0816 16:51:20.438746   91938 system_pods.go:61] "etcd-minikube" [06865e18-f33f-46d5-977a-945f42ddc4df] Running / Ready:ContainersNotReady (containers with unready status: [etcd]) / ContainersReady:ContainersNotReady (containers with unready status: [etcd])
I0816 16:51:20.438750   91938 system_pods.go:61] "kube-apiserver-minikube" [705db576-bf67-4f47-b877-f6a47d7790a8] Running / Ready:ContainersNotReady (containers with unready status: [kube-apiserver]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-apiserver])
I0816 16:51:20.438753   91938 system_pods.go:61] "kube-controller-manager-minikube" [da3828d3-a724-4956-bba2-9b917652a9dc] Running / Ready:ContainersNotReady (containers with unready status: [kube-controller-manager]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-controller-manager])
I0816 16:51:20.438756   91938 system_pods.go:61] "kube-proxy-ncbgc" [21ec3cdc-e50b-4a2c-8d6f-2b0fc2b1eb3c] Running / Ready:ContainersNotReady (containers with unready status: [kube-proxy]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-proxy])
I0816 16:51:20.438758   91938 system_pods.go:61] "kube-scheduler-minikube" [4fa25efb-5b4d-47e2-aec0-e21d86e5aa42] Running / Ready:ContainersNotReady (containers with unready status: [kube-scheduler]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-scheduler])
I0816 16:51:20.438770   91938 system_pods.go:61] "metrics-server-844d8db974-z5wrd" [4b6d6190-223d-478c-98dd-4634bd1a4cee] Running / Ready:ContainersNotReady (containers with unready status: [metrics-server]) / ContainersReady:ContainersNotReady (containers with unready status: [metrics-server])
I0816 16:51:20.438772   91938 system_pods.go:61] "storage-provisioner" [46ca2804-7b31-40bc-a389-1829ebcdb03c] Running / Ready:ContainersNotReady (containers with unready status: [storage-provisioner]) / ContainersReady:ContainersNotReady (containers with unready status: [storage-provisioner])
I0816 16:51:20.438774   91938 system_pods.go:74] duration metric: took 16.724917ms to wait for pod list to return data ...
I0816 16:51:20.438778   91938 node_conditions.go:102] verifying NodePressure condition ...
I0816 16:51:20.441615   91938 node_conditions.go:122] node storage ephemeral capacity is 17784760Ki
I0816 16:51:20.441662   91938 node_conditions.go:123] node cpu capacity is 2
I0816 16:51:20.441708   91938 node_conditions.go:105] duration metric: took 2.791459ms to run NodePressure ...
I0816 16:51:20.441962   91938 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.27.3:$PATH" kubeadm init phase addon all --config /var/tmp/minikube/kubeadm.yaml"
I0816 16:51:20.554602   91938 ssh_runner.go:195] Run: /bin/bash -c "cat /proc/$(pgrep kube-apiserver)/oom_adj"
I0816 16:51:20.559938   91938 ops.go:34] apiserver oom_adj: -16
I0816 16:51:20.559944   91938 kubeadm.go:640] restartCluster took 16.968307625s
I0816 16:51:20.559947   91938 kubeadm.go:406] StartCluster complete in 16.981044125s
I0816 16:51:20.560305   91938 settings.go:142] acquiring lock: {Name:mk6d46b3abd23c24e265f74d7bad755a1c27813f Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0816 16:51:20.561735   91938 settings.go:150] Updating kubeconfig:  /Users/kuldeep/.kube/config
I0816 16:51:20.571595   91938 lock.go:35] WriteFile acquiring /Users/kuldeep/.kube/config: {Name:mk3b13c8606d7900dd0a50d6ef0deaaa4a35d161 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0816 16:51:20.573120   91938 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.27.3/kubectl --kubeconfig=/var/lib/minikube/kubeconfig -n kube-system get configmap coredns -o yaml"
I0816 16:51:20.573815   91938 addons.go:499] enable addons start: toEnable=map[ambassador:false auto-pause:false cloud-spanner:false csi-hostpath-driver:false dashboard:true default-storageclass:true efk:false freshpod:false gcp-auth:false gvisor:false headlamp:false helm-tiller:false inaccel:false ingress:false ingress-dns:false inspektor-gadget:false istio:false istio-provisioner:false kong:false kubevirt:false logviewer:false metallb:false metrics-server:true nvidia-driver-installer:false nvidia-gpu-device-plugin:false olm:false pod-security-policy:false portainer:false registry:false registry-aliases:false registry-creds:false storage-provisioner:true storage-provisioner-gluster:false volumesnapshots:false]
I0816 16:51:20.573977   91938 addons.go:69] Setting storage-provisioner=true in profile "minikube"
I0816 16:51:20.573983   91938 addons.go:231] Setting addon storage-provisioner=true in "minikube"
W0816 16:51:20.573986   91938 addons.go:240] addon storage-provisioner should already be in state true
I0816 16:51:20.573994   91938 addons.go:69] Setting dashboard=true in profile "minikube"
I0816 16:51:20.574002   91938 addons.go:231] Setting addon dashboard=true in "minikube"
W0816 16:51:20.574005   91938 addons.go:240] addon dashboard should already be in state true
I0816 16:51:20.573998   91938 addons.go:69] Setting metrics-server=true in profile "minikube"
I0816 16:51:20.574013   91938 addons.go:231] Setting addon metrics-server=true in "minikube"
W0816 16:51:20.574015   91938 addons.go:240] addon metrics-server should already be in state true
I0816 16:51:20.574018   91938 addons.go:69] Setting default-storageclass=true in profile "minikube"
I0816 16:51:20.574036   91938 addons_storage_classes.go:33] enableOrDisableStorageClasses default-storageclass=true on "minikube"
I0816 16:51:20.574573   91938 config.go:182] Loaded profile config "minikube": Driver=qemu2, ContainerRuntime=docker, KubernetesVersion=v1.27.3
I0816 16:51:20.575106   91938 host.go:66] Checking if "minikube" exists ...
I0816 16:51:20.575111   91938 host.go:66] Checking if "minikube" exists ...
I0816 16:51:20.575149   91938 host.go:66] Checking if "minikube" exists ...
W0816 16:51:20.576432   91938 host.go:54] host status for "minikube" returned error: state: connect: dial unix /Users/kuldeep/.minikube/machines/minikube/monitor: connect: connection refused
W0816 16:51:20.576440   91938 host.go:54] host status for "minikube" returned error: state: connect: dial unix /Users/kuldeep/.minikube/machines/minikube/monitor: connect: connection refused
W0816 16:51:20.576595   91938 addons.go:277] "minikube" is not running, setting metrics-server=true and skipping enablement (err=<nil>)
W0816 16:51:20.576595   91938 addons.go:277] "minikube" is not running, setting dashboard=true and skipping enablement (err=<nil>)
I0816 16:51:20.576608   91938 addons.go:467] Verifying addon metrics-server=true in "minikube"
I0816 16:51:20.583222   91938 out.go:177] üí°  Some dashboard features require the metrics-server addon. To enable all features please run:

	minikube addons enable metrics-server	


I0816 16:51:20.586176   91938 kapi.go:248] "coredns" deployment in "kube-system" namespace and "minikube" context rescaled to 1 replicas
I0816 16:51:20.588475   91938 addons.go:231] Setting addon default-storageclass=true in "minikube"
W0816 16:51:20.591791   91938 addons.go:240] addon default-storageclass should already be in state true
I0816 16:51:20.591792   91938 out.go:177]     ‚ñ™ Using image gcr.io/k8s-minikube/storage-provisioner:v5
I0816 16:51:20.591808   91938 host.go:66] Checking if "minikube" exists ...
I0816 16:51:20.591988   91938 start.go:223] Will wait 6m0s for node &{Name: IP:192.168.105.2 Port:8443 KubernetesVersion:v1.27.3 ContainerRuntime:docker ControlPlane:true Worker:true}
I0816 16:51:20.601951   91938 out.go:177] üîé  Verifying Kubernetes components...
I0816 16:51:20.597885   91938 addons.go:423] installing /etc/kubernetes/addons/storage-provisioner.yaml
I0816 16:51:20.597925   91938 addons.go:423] installing /etc/kubernetes/addons/storageclass.yaml
I0816 16:51:20.605000   91938 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/storageclass.yaml (271 bytes)
I0816 16:51:20.605013   91938 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/storage-provisioner.yaml (2676 bytes)
I0816 16:51:20.605148   91938 sshutil.go:53] new ssh client: &{IP:192.168.105.2 Port:22 SSHKeyPath:/Users/kuldeep/.minikube/machines/minikube/id_rsa Username:docker}
I0816 16:51:20.605148   91938 sshutil.go:53] new ssh client: &{IP:192.168.105.2 Port:22 SSHKeyPath:/Users/kuldeep/.minikube/machines/minikube/id_rsa Username:docker}
I0816 16:51:20.605156   91938 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service kubelet
I0816 16:51:20.662523   91938 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.27.3/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml
I0816 16:51:20.679080   91938 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.27.3/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml
I0816 16:51:20.788285   91938 api_server.go:52] waiting for apiserver process to appear ...
I0816 16:51:20.788465   91938 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0816 16:51:20.789185   91938 start.go:874] CoreDNS already contains "host.minikube.internal" host record, skipping...
I0816 16:51:50.223915   91938 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.27.3/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml: (29.561025083s)
I0816 16:51:50.223974   91938 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.27.3/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml: (29.544517167s)
I0816 16:51:50.224014   91938 ssh_runner.go:235] Completed: sudo pgrep -xnf kube-apiserver.*minikube.*: (29.435218583s)
I0816 16:51:50.224038   91938 api_server.go:72] duration metric: took 29.626644542s to wait for apiserver process to appear ...
I0816 16:51:50.224049   91938 api_server.go:88] waiting for apiserver healthz status ...
I0816 16:51:50.224079   91938 api_server.go:253] Checking apiserver healthz at https://192.168.105.2:8443/healthz ...
I0816 16:51:50.229269   91938 out.go:177] üåü  Enabled addons: metrics-server, dashboard, storage-provisioner, default-storageclass
I0816 16:51:50.229224   91938 api_server.go:279] https://192.168.105.2:8443/healthz returned 200:
ok
I0816 16:51:50.230628   91938 api_server.go:141] control plane version: v1.27.3
I0816 16:51:50.236190   91938 addons.go:502] enable addons completed in 29.662404041s: enabled=[metrics-server dashboard storage-provisioner default-storageclass]
I0816 16:51:50.236208   91938 api_server.go:131] duration metric: took 12.152875ms to wait for apiserver health ...
I0816 16:51:50.236215   91938 system_pods.go:43] waiting for kube-system pods to appear ...
I0816 16:51:50.247028   91938 system_pods.go:59] 8 kube-system pods found
I0816 16:51:50.247036   91938 system_pods.go:61] "coredns-5d78c9869d-ptqjm" [05e0c7e4-0c61-4fa0-a77c-ce1f35365062] Running
I0816 16:51:50.247038   91938 system_pods.go:61] "etcd-minikube" [06865e18-f33f-46d5-977a-945f42ddc4df] Running
I0816 16:51:50.247040   91938 system_pods.go:61] "kube-apiserver-minikube" [705db576-bf67-4f47-b877-f6a47d7790a8] Running
I0816 16:51:50.247042   91938 system_pods.go:61] "kube-controller-manager-minikube" [da3828d3-a724-4956-bba2-9b917652a9dc] Running
I0816 16:51:50.247043   91938 system_pods.go:61] "kube-proxy-ncbgc" [21ec3cdc-e50b-4a2c-8d6f-2b0fc2b1eb3c] Running
I0816 16:51:50.247045   91938 system_pods.go:61] "kube-scheduler-minikube" [4fa25efb-5b4d-47e2-aec0-e21d86e5aa42] Running
I0816 16:51:50.247047   91938 system_pods.go:61] "metrics-server-844d8db974-z5wrd" [4b6d6190-223d-478c-98dd-4634bd1a4cee] Running
I0816 16:51:50.247050   91938 system_pods.go:61] "storage-provisioner" [46ca2804-7b31-40bc-a389-1829ebcdb03c] Running
I0816 16:51:50.247052   91938 system_pods.go:74] duration metric: took 10.833708ms to wait for pod list to return data ...
I0816 16:51:50.247060   91938 kubeadm.go:581] duration metric: took 29.649667625s to wait for : map[apiserver:true system_pods:true] ...
I0816 16:51:50.247316   91938 node_conditions.go:102] verifying NodePressure condition ...
I0816 16:51:50.250273   91938 node_conditions.go:122] node storage ephemeral capacity is 17784760Ki
I0816 16:51:50.250292   91938 node_conditions.go:123] node cpu capacity is 2
I0816 16:51:50.250299   91938 node_conditions.go:105] duration metric: took 2.97875ms to run NodePressure ...
I0816 16:51:50.250309   91938 start.go:228] waiting for startup goroutines ...
I0816 16:51:50.250313   91938 start.go:233] waiting for cluster config update ...
I0816 16:51:50.250323   91938 start.go:242] writing updated cluster config ...
I0816 16:51:50.252582   91938 ssh_runner.go:195] Run: rm -f paused
I0816 16:51:50.411171   91938 start.go:596] kubectl: 1.27.4, cluster: 1.27.3 (minor skew: 0)
I0816 16:51:50.416068   91938 out.go:177] üèÑ  Done! kubectl is now configured to use "minikube" cluster and "default" namespace by default

* 
* ==> Docker <==
* -- Journal begins at Wed 2023-08-16 15:50:37 UTC, ends at Wed 2023-08-16 15:59:23 UTC. --
Aug 16 15:51:25 minikube cri-dockerd[992]: time="2023-08-16T15:51:25Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/05ed3ba27b8dd5de83862ddaeecb959cfd70467ecafe54d174dc1b4708e3bb31/resolv.conf as [nameserver 10.96.0.10 search kube-system.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Aug 16 15:51:26 minikube dockerd[795]: time="2023-08-16T15:51:26.039499525Z" level=info msg="loading plugin \"io.containerd.internal.v1.shutdown\"..." runtime=io.containerd.runc.v2 type=io.containerd.internal.v1
Aug 16 15:51:26 minikube dockerd[795]: time="2023-08-16T15:51:26.039532025Z" level=info msg="loading plugin \"io.containerd.ttrpc.v1.pause\"..." runtime=io.containerd.runc.v2 type=io.containerd.ttrpc.v1
Aug 16 15:51:26 minikube dockerd[795]: time="2023-08-16T15:51:26.039539150Z" level=info msg="loading plugin \"io.containerd.event.v1.publisher\"..." runtime=io.containerd.runc.v2 type=io.containerd.event.v1
Aug 16 15:51:26 minikube dockerd[795]: time="2023-08-16T15:51:26.039543442Z" level=info msg="loading plugin \"io.containerd.ttrpc.v1.task\"..." runtime=io.containerd.runc.v2 type=io.containerd.ttrpc.v1
Aug 16 15:51:27 minikube dockerd[795]: time="2023-08-16T15:51:27.597066359Z" level=info msg="loading plugin \"io.containerd.internal.v1.shutdown\"..." runtime=io.containerd.runc.v2 type=io.containerd.internal.v1
Aug 16 15:51:27 minikube dockerd[795]: time="2023-08-16T15:51:27.597124734Z" level=info msg="loading plugin \"io.containerd.ttrpc.v1.pause\"..." runtime=io.containerd.runc.v2 type=io.containerd.ttrpc.v1
Aug 16 15:51:27 minikube dockerd[795]: time="2023-08-16T15:51:27.597142109Z" level=info msg="loading plugin \"io.containerd.event.v1.publisher\"..." runtime=io.containerd.runc.v2 type=io.containerd.event.v1
Aug 16 15:51:27 minikube dockerd[795]: time="2023-08-16T15:51:27.597151776Z" level=info msg="loading plugin \"io.containerd.ttrpc.v1.task\"..." runtime=io.containerd.runc.v2 type=io.containerd.ttrpc.v1
Aug 16 15:51:27 minikube dockerd[795]: time="2023-08-16T15:51:27.604846859Z" level=info msg="loading plugin \"io.containerd.internal.v1.shutdown\"..." runtime=io.containerd.runc.v2 type=io.containerd.internal.v1
Aug 16 15:51:27 minikube dockerd[795]: time="2023-08-16T15:51:27.605055943Z" level=info msg="loading plugin \"io.containerd.ttrpc.v1.pause\"..." runtime=io.containerd.runc.v2 type=io.containerd.ttrpc.v1
Aug 16 15:51:27 minikube dockerd[795]: time="2023-08-16T15:51:27.605063943Z" level=info msg="loading plugin \"io.containerd.event.v1.publisher\"..." runtime=io.containerd.runc.v2 type=io.containerd.event.v1
Aug 16 15:51:27 minikube dockerd[795]: time="2023-08-16T15:51:27.605068693Z" level=info msg="loading plugin \"io.containerd.ttrpc.v1.task\"..." runtime=io.containerd.runc.v2 type=io.containerd.ttrpc.v1
Aug 16 15:51:27 minikube dockerd[795]: time="2023-08-16T15:51:27.609784109Z" level=info msg="loading plugin \"io.containerd.internal.v1.shutdown\"..." runtime=io.containerd.runc.v2 type=io.containerd.internal.v1
Aug 16 15:51:27 minikube dockerd[795]: time="2023-08-16T15:51:27.609847443Z" level=info msg="loading plugin \"io.containerd.ttrpc.v1.pause\"..." runtime=io.containerd.runc.v2 type=io.containerd.ttrpc.v1
Aug 16 15:51:27 minikube dockerd[795]: time="2023-08-16T15:51:27.609861734Z" level=info msg="loading plugin \"io.containerd.event.v1.publisher\"..." runtime=io.containerd.runc.v2 type=io.containerd.event.v1
Aug 16 15:51:27 minikube dockerd[795]: time="2023-08-16T15:51:27.609870984Z" level=info msg="loading plugin \"io.containerd.ttrpc.v1.task\"..." runtime=io.containerd.runc.v2 type=io.containerd.ttrpc.v1
Aug 16 15:51:27 minikube cri-dockerd[992]: time="2023-08-16T15:51:27Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/ea24fdb508b42181937496f0c9652785006a0028bfed99e1f1cf621a82010642/resolv.conf as [nameserver 192.168.105.1]"
Aug 16 15:51:27 minikube dockerd[795]: time="2023-08-16T15:51:27.881624984Z" level=info msg="loading plugin \"io.containerd.internal.v1.shutdown\"..." runtime=io.containerd.runc.v2 type=io.containerd.internal.v1
Aug 16 15:51:27 minikube dockerd[795]: time="2023-08-16T15:51:27.881677318Z" level=info msg="loading plugin \"io.containerd.ttrpc.v1.pause\"..." runtime=io.containerd.runc.v2 type=io.containerd.ttrpc.v1
Aug 16 15:51:27 minikube dockerd[795]: time="2023-08-16T15:51:27.881684151Z" level=info msg="loading plugin \"io.containerd.event.v1.publisher\"..." runtime=io.containerd.runc.v2 type=io.containerd.event.v1
Aug 16 15:51:27 minikube dockerd[795]: time="2023-08-16T15:51:27.881688484Z" level=info msg="loading plugin \"io.containerd.ttrpc.v1.task\"..." runtime=io.containerd.runc.v2 type=io.containerd.ttrpc.v1
Aug 16 15:51:27 minikube cri-dockerd[992]: time="2023-08-16T15:51:27Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/4ff440415069941dc67c590fee84dc93f95d0712ddbb76b82c20095fc4d849ae/resolv.conf as [nameserver 10.96.0.10 search kubernetes-dashboard.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Aug 16 15:51:27 minikube cri-dockerd[992]: time="2023-08-16T15:51:27Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/3d9d08adce7a742a0a1e2f3fbca6da6a54efcea426edf4459a1d3a1134ee3418/resolv.conf as [nameserver 10.96.0.10 search kubernetes-dashboard.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Aug 16 15:51:28 minikube dockerd[795]: time="2023-08-16T15:51:28.004439109Z" level=info msg="loading plugin \"io.containerd.internal.v1.shutdown\"..." runtime=io.containerd.runc.v2 type=io.containerd.internal.v1
Aug 16 15:51:28 minikube dockerd[795]: time="2023-08-16T15:51:28.004758943Z" level=info msg="loading plugin \"io.containerd.ttrpc.v1.pause\"..." runtime=io.containerd.runc.v2 type=io.containerd.ttrpc.v1
Aug 16 15:51:28 minikube dockerd[795]: time="2023-08-16T15:51:28.004772901Z" level=info msg="loading plugin \"io.containerd.event.v1.publisher\"..." runtime=io.containerd.runc.v2 type=io.containerd.event.v1
Aug 16 15:51:28 minikube dockerd[795]: time="2023-08-16T15:51:28.004780193Z" level=info msg="loading plugin \"io.containerd.ttrpc.v1.task\"..." runtime=io.containerd.runc.v2 type=io.containerd.ttrpc.v1
Aug 16 15:51:28 minikube dockerd[795]: time="2023-08-16T15:51:28.030700859Z" level=info msg="loading plugin \"io.containerd.internal.v1.shutdown\"..." runtime=io.containerd.runc.v2 type=io.containerd.internal.v1
Aug 16 15:51:28 minikube dockerd[795]: time="2023-08-16T15:51:28.030775901Z" level=info msg="loading plugin \"io.containerd.ttrpc.v1.pause\"..." runtime=io.containerd.runc.v2 type=io.containerd.ttrpc.v1
Aug 16 15:51:28 minikube dockerd[795]: time="2023-08-16T15:51:28.030793859Z" level=info msg="loading plugin \"io.containerd.event.v1.publisher\"..." runtime=io.containerd.runc.v2 type=io.containerd.event.v1
Aug 16 15:51:28 minikube dockerd[795]: time="2023-08-16T15:51:28.030806651Z" level=info msg="loading plugin \"io.containerd.ttrpc.v1.task\"..." runtime=io.containerd.runc.v2 type=io.containerd.ttrpc.v1
Aug 16 15:51:50 minikube dockerd[789]: time="2023-08-16T15:51:50.379073820Z" level=info msg="ignoring event" container=057ac53ae0884a9ce6e39145f05aedbe54e6350b9ce6ce596a214ca881c9cfb2 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Aug 16 15:51:50 minikube dockerd[795]: time="2023-08-16T15:51:50.379428709Z" level=info msg="shim disconnected" id=057ac53ae0884a9ce6e39145f05aedbe54e6350b9ce6ce596a214ca881c9cfb2 namespace=moby
Aug 16 15:51:50 minikube dockerd[795]: time="2023-08-16T15:51:50.379617777Z" level=warning msg="cleaning up after shim disconnected" id=057ac53ae0884a9ce6e39145f05aedbe54e6350b9ce6ce596a214ca881c9cfb2 namespace=moby
Aug 16 15:51:50 minikube dockerd[795]: time="2023-08-16T15:51:50.379637442Z" level=info msg="cleaning up dead shim" namespace=moby
Aug 16 15:52:02 minikube dockerd[795]: time="2023-08-16T15:52:02.826967911Z" level=info msg="loading plugin \"io.containerd.internal.v1.shutdown\"..." runtime=io.containerd.runc.v2 type=io.containerd.internal.v1
Aug 16 15:52:02 minikube dockerd[795]: time="2023-08-16T15:52:02.827099198Z" level=info msg="loading plugin \"io.containerd.ttrpc.v1.pause\"..." runtime=io.containerd.runc.v2 type=io.containerd.ttrpc.v1
Aug 16 15:52:02 minikube dockerd[795]: time="2023-08-16T15:52:02.827113031Z" level=info msg="loading plugin \"io.containerd.event.v1.publisher\"..." runtime=io.containerd.runc.v2 type=io.containerd.event.v1
Aug 16 15:52:02 minikube dockerd[795]: time="2023-08-16T15:52:02.828106953Z" level=info msg="loading plugin \"io.containerd.ttrpc.v1.task\"..." runtime=io.containerd.runc.v2 type=io.containerd.ttrpc.v1
Aug 16 15:52:16 minikube dockerd[795]: time="2023-08-16T15:52:16.401467116Z" level=info msg="loading plugin \"io.containerd.internal.v1.shutdown\"..." runtime=io.containerd.runc.v2 type=io.containerd.internal.v1
Aug 16 15:52:16 minikube dockerd[795]: time="2023-08-16T15:52:16.402666474Z" level=info msg="loading plugin \"io.containerd.ttrpc.v1.pause\"..." runtime=io.containerd.runc.v2 type=io.containerd.ttrpc.v1
Aug 16 15:52:16 minikube dockerd[795]: time="2023-08-16T15:52:16.402684057Z" level=info msg="loading plugin \"io.containerd.event.v1.publisher\"..." runtime=io.containerd.runc.v2 type=io.containerd.event.v1
Aug 16 15:52:16 minikube dockerd[795]: time="2023-08-16T15:52:16.402721889Z" level=info msg="loading plugin \"io.containerd.ttrpc.v1.task\"..." runtime=io.containerd.runc.v2 type=io.containerd.ttrpc.v1
Aug 16 15:52:16 minikube cri-dockerd[992]: time="2023-08-16T15:52:16Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/c757477817a2654d1fdf32708cfcedb395dd8a21165963acfb6d7123cca82c6a/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Aug 16 15:52:23 minikube cri-dockerd[992]: time="2023-08-16T15:52:23Z" level=info msg="Stop pulling image kdmalviyan/expense-service:latest: Status: Downloaded newer image for kdmalviyan/expense-service:latest"
Aug 16 15:52:23 minikube dockerd[795]: time="2023-08-16T15:52:23.202455261Z" level=info msg="loading plugin \"io.containerd.internal.v1.shutdown\"..." runtime=io.containerd.runc.v2 type=io.containerd.internal.v1
Aug 16 15:52:23 minikube dockerd[795]: time="2023-08-16T15:52:23.203200213Z" level=info msg="loading plugin \"io.containerd.ttrpc.v1.pause\"..." runtime=io.containerd.runc.v2 type=io.containerd.ttrpc.v1
Aug 16 15:52:23 minikube dockerd[795]: time="2023-08-16T15:52:23.203216338Z" level=info msg="loading plugin \"io.containerd.event.v1.publisher\"..." runtime=io.containerd.runc.v2 type=io.containerd.event.v1
Aug 16 15:52:23 minikube dockerd[795]: time="2023-08-16T15:52:23.203221588Z" level=info msg="loading plugin \"io.containerd.ttrpc.v1.task\"..." runtime=io.containerd.runc.v2 type=io.containerd.ttrpc.v1
Aug 16 15:56:04 minikube dockerd[795]: time="2023-08-16T15:56:04.608802055Z" level=info msg="loading plugin \"io.containerd.internal.v1.shutdown\"..." runtime=io.containerd.runc.v2 type=io.containerd.internal.v1
Aug 16 15:56:04 minikube dockerd[795]: time="2023-08-16T15:56:04.608902224Z" level=info msg="loading plugin \"io.containerd.ttrpc.v1.pause\"..." runtime=io.containerd.runc.v2 type=io.containerd.ttrpc.v1
Aug 16 15:56:04 minikube dockerd[795]: time="2023-08-16T15:56:04.608922516Z" level=info msg="loading plugin \"io.containerd.event.v1.publisher\"..." runtime=io.containerd.runc.v2 type=io.containerd.event.v1
Aug 16 15:56:04 minikube dockerd[795]: time="2023-08-16T15:56:04.608931850Z" level=info msg="loading plugin \"io.containerd.ttrpc.v1.task\"..." runtime=io.containerd.runc.v2 type=io.containerd.ttrpc.v1
Aug 16 15:56:04 minikube cri-dockerd[992]: time="2023-08-16T15:56:04Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/bcf6fbfaeb7aeb181bc93951443df1b5a70eae255a397629170c05fdad438a16/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Aug 16 15:56:11 minikube cri-dockerd[992]: time="2023-08-16T15:56:11Z" level=info msg="Stop pulling image kdmalviyan/user-service:latest: Status: Downloaded newer image for kdmalviyan/user-service:latest"
Aug 16 15:56:11 minikube dockerd[795]: time="2023-08-16T15:56:11.525601742Z" level=info msg="loading plugin \"io.containerd.internal.v1.shutdown\"..." runtime=io.containerd.runc.v2 type=io.containerd.internal.v1
Aug 16 15:56:11 minikube dockerd[795]: time="2023-08-16T15:56:11.525674827Z" level=info msg="loading plugin \"io.containerd.ttrpc.v1.pause\"..." runtime=io.containerd.runc.v2 type=io.containerd.ttrpc.v1
Aug 16 15:56:11 minikube dockerd[795]: time="2023-08-16T15:56:11.525684453Z" level=info msg="loading plugin \"io.containerd.event.v1.publisher\"..." runtime=io.containerd.runc.v2 type=io.containerd.event.v1
Aug 16 15:56:11 minikube dockerd[795]: time="2023-08-16T15:56:11.525694453Z" level=info msg="loading plugin \"io.containerd.ttrpc.v1.task\"..." runtime=io.containerd.runc.v2 type=io.containerd.ttrpc.v1

* 
* ==> container status <==
* CONTAINER           IMAGE                                                                                                CREATED             STATE               NAME                             ATTEMPT             POD ID
f868c1c1fcbc7       kdmalviyan/user-service@sha256:0e6ce2009867d98fc25993fb913e92349af169a5482bf2686d2ab60c6309e91b      3 minutes ago       Running             user-service-app-deployment      0                   bcf6fbfaeb7ae
d449ea7c9cf7f       kdmalviyan/expense-service@sha256:ef25a33967a57d92d958c8c01c52d13c77684778308447adb1b10f78f01907c5   7 minutes ago       Running             expense-service-app-deployment   0                   c757477817a26
5aa86e72869d3       ba04bb24b9575                                                                                        7 minutes ago       Running             storage-provisioner              4                   72fc6cfebb267
af86ca725b5ef       20b332c9a70d8                                                                                        7 minutes ago       Running             kubernetes-dashboard             2                   3d9d08adce7a7
01adaa6c29ee8       a422e0e982356                                                                                        7 minutes ago       Running             dashboard-metrics-scraper        2                   4ff4404150699
54748eea7e44c       97e04611ad434                                                                                        7 minutes ago       Running             coredns                          2                   ea24fdb508b42
0f8c1990de495       8e22bf689cda7                                                                                        7 minutes ago       Running             metrics-server                   2                   05ed3ba27b8dd
057ac53ae0884       ba04bb24b9575                                                                                        8 minutes ago       Exited              storage-provisioner              3                   72fc6cfebb267
3bf25d7c0f8e6       fb73e92641fd5                                                                                        8 minutes ago       Running             kube-proxy                       2                   ae018000bec72
5eaa85cbd11fc       39dfb036b0986                                                                                        8 minutes ago       Running             kube-apiserver                   2                   334290fc8997a
6ed612f2be8c6       bcb9e554eaab6                                                                                        8 minutes ago       Running             kube-scheduler                   2                   fe43f90b15477
02b54c8b20dc8       24bc64e911039                                                                                        8 minutes ago       Running             etcd                             2                   df489c06fee81
579d691ede2f4       ab3683b584ae5                                                                                        8 minutes ago       Running             kube-controller-manager          2                   d575ab51fbafd
27b7f95cf482a       a422e0e982356                                                                                        5 hours ago         Exited              dashboard-metrics-scraper        1                   f96e3ad276bd1
e54576a9ff17b       20b332c9a70d8                                                                                        5 hours ago         Exited              kubernetes-dashboard             1                   c332ae7646c99
e25e7bb53be20       97e04611ad434                                                                                        5 hours ago         Exited              coredns                          1                   fb28c45afa76d
63855b84420c7       8e22bf689cda7                                                                                        5 hours ago         Exited              metrics-server                   1                   6da0cdf7457ce
b0f5f807e4cf1       fb73e92641fd5                                                                                        5 hours ago         Exited              kube-proxy                       1                   cd09005e9e06c
5a9deb8a0f844       24bc64e911039                                                                                        5 hours ago         Exited              etcd                             1                   00f4858c41159
32d50646391c3       39dfb036b0986                                                                                        5 hours ago         Exited              kube-apiserver                   1                   dfa3cdd416b23
1f3795d91eeb5       bcb9e554eaab6                                                                                        5 hours ago         Exited              kube-scheduler                   1                   7ffb4df97592f
31be563eb51c4       ab3683b584ae5                                                                                        5 hours ago         Exited              kube-controller-manager          1                   cba4066607c80

* 
* ==> coredns [54748eea7e44] <==
* .:53
[INFO] plugin/reload: Running configuration SHA512 = ea7a0d73d9d208f758b1f67640ef03c58089b9d9366cf3478df3bb369b210e39f213811b46224f8a04380814b6e0890ccd358f5b5e8c80bc22ac19c8601ee35b
CoreDNS-1.10.1
linux/arm64, go1.20, 055b2c3
[INFO] 127.0.0.1:50509 - 62478 "HINFO IN 7253617841304650319.3425433303654797928. udp 57 false 512" NXDOMAIN qr,rd,ra 132 0.027184208s

* 
* ==> coredns [e25e7bb53be2] <==
* [INFO] plugin/ready: Still waiting on: "kubernetes"
.:53
[INFO] plugin/reload: Running configuration SHA512 = ea7a0d73d9d208f758b1f67640ef03c58089b9d9366cf3478df3bb369b210e39f213811b46224f8a04380814b6e0890ccd358f5b5e8c80bc22ac19c8601ee35b
CoreDNS-1.10.1
linux/arm64, go1.20, 055b2c3
[INFO] 127.0.0.1:51732 - 40284 "HINFO IN 7615451201790456595.3473294100906195196. udp 57 false 512" NXDOMAIN qr,rd,ra 132 0.024701476s

* 
* ==> describe nodes <==
* Name:               minikube
Roles:              control-plane
Labels:             beta.kubernetes.io/arch=arm64
                    beta.kubernetes.io/os=linux
                    kubernetes.io/arch=arm64
                    kubernetes.io/hostname=minikube
                    kubernetes.io/os=linux
                    minikube.k8s.io/commit=fd3f3801765d093a485d255043149f92ec0a695f
                    minikube.k8s.io/name=minikube
                    minikube.k8s.io/primary=true
                    minikube.k8s.io/updated_at=2023_08_08T17_53_49_0700
                    minikube.k8s.io/version=v1.31.1
                    node-role.kubernetes.io/control-plane=
                    node.kubernetes.io/exclude-from-external-load-balancers=
Annotations:        kubeadm.alpha.kubernetes.io/cri-socket: unix:///var/run/cri-dockerd.sock
                    node.alpha.kubernetes.io/ttl: 0
                    volumes.kubernetes.io/controller-managed-attach-detach: true
CreationTimestamp:  Tue, 08 Aug 2023 16:53:46 +0000
Taints:             <none>
Unschedulable:      false
Lease:
  HolderIdentity:  minikube
  AcquireTime:     <unset>
  RenewTime:       Wed, 16 Aug 2023 15:59:18 +0000
Conditions:
  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message
  ----             ------  -----------------                 ------------------                ------                       -------
  MemoryPressure   False   Wed, 16 Aug 2023 15:56:15 +0000   Tue, 08 Aug 2023 16:53:44 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available
  DiskPressure     False   Wed, 16 Aug 2023 15:56:15 +0000   Tue, 08 Aug 2023 16:53:44 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure
  PIDPressure      False   Wed, 16 Aug 2023 15:56:15 +0000   Tue, 08 Aug 2023 16:53:44 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available
  Ready            True    Wed, 16 Aug 2023 15:56:15 +0000   Wed, 16 Aug 2023 15:51:29 +0000   KubeletReady                 kubelet is posting ready status
Addresses:
  InternalIP:  192.168.105.2
  Hostname:    minikube
Capacity:
  cpu:                2
  ephemeral-storage:  17784760Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  hugepages-32Mi:     0
  hugepages-64Ki:     0
  memory:             2147724Ki
  pods:               110
Allocatable:
  cpu:                2
  ephemeral-storage:  17784760Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  hugepages-32Mi:     0
  hugepages-64Ki:     0
  memory:             2147724Ki
  pods:               110
System Info:
  Machine ID:                 4fbfc7043a664acda30300ebcec694ab
  System UUID:                4fbfc7043a664acda30300ebcec694ab
  Boot ID:                    eb5734d8-f493-4ec7-8830-925da45ab795
  Kernel Version:             5.10.57
  OS Image:                   Buildroot 2021.02.12
  Operating System:           linux
  Architecture:               arm64
  Container Runtime Version:  docker://24.0.4
  Kubelet Version:            v1.27.3
  Kube-Proxy Version:         v1.27.3
PodCIDR:                      10.244.0.0/24
PodCIDRs:                     10.244.0.0/24
Non-terminated Pods:          (12 in total)
  Namespace                   Name                                              CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age
  ---------                   ----                                              ------------  ----------  ---------------  -------------  ---
  default                     expense-service-app-deployment-7dfd987bb-5t45r    0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         7m8s
  default                     user-service-app-deployment-6869bd8cc5-4hsdv      0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         3m19s
  kube-system                 coredns-5d78c9869d-ptqjm                          100m (5%!)(MISSING)     0 (0%!)(MISSING)      70Mi (3%!)(MISSING)        170Mi (8%!)(MISSING)     7d23h
  kube-system                 etcd-minikube                                     100m (5%!)(MISSING)     0 (0%!)(MISSING)      100Mi (4%!)(MISSING)       0 (0%!)(MISSING)         7d23h
  kube-system                 kube-apiserver-minikube                           250m (12%!)(MISSING)    0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         7d23h
  kube-system                 kube-controller-manager-minikube                  200m (10%!)(MISSING)    0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         7d23h
  kube-system                 kube-proxy-ncbgc                                  0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         7d23h
  kube-system                 kube-scheduler-minikube                           100m (5%!)(MISSING)     0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         7d23h
  kube-system                 metrics-server-844d8db974-z5wrd                   100m (5%!)(MISSING)     0 (0%!)(MISSING)      200Mi (9%!)(MISSING)       0 (0%!)(MISSING)         7d23h
  kube-system                 storage-provisioner                               0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         7d23h
  kubernetes-dashboard        dashboard-metrics-scraper-5dd9cbfd69-nfdf8        0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         7d23h
  kubernetes-dashboard        kubernetes-dashboard-5c5cfc8747-sq86c             0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         7d23h
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  Resource           Requests     Limits
  --------           --------     ------
  cpu                850m (42%!)(MISSING)   0 (0%!)(MISSING)
  memory             370Mi (17%!)(MISSING)  170Mi (8%!)(MISSING)
  ephemeral-storage  0 (0%!)(MISSING)       0 (0%!)(MISSING)
  hugepages-1Gi      0 (0%!)(MISSING)       0 (0%!)(MISSING)
  hugepages-2Mi      0 (0%!)(MISSING)       0 (0%!)(MISSING)
  hugepages-32Mi     0 (0%!)(MISSING)       0 (0%!)(MISSING)
  hugepages-64Ki     0 (0%!)(MISSING)       0 (0%!)(MISSING)
Events:
  Type    Reason                   Age                  From             Message
  ----    ------                   ----                 ----             -------
  Normal  Starting                 5h4m                 kube-proxy       
  Normal  Starting                 8m3s                 kube-proxy       
  Normal  NodeAllocatableEnforced  5h4m                 kubelet          Updated Node Allocatable limit across pods
  Normal  NodeHasSufficientMemory  5h4m (x8 over 5h4m)  kubelet          Node minikube status is now: NodeHasSufficientMemory
  Normal  NodeHasNoDiskPressure    5h4m (x8 over 5h4m)  kubelet          Node minikube status is now: NodeHasNoDiskPressure
  Normal  NodeHasSufficientPID     5h4m (x7 over 5h4m)  kubelet          Node minikube status is now: NodeHasSufficientPID
  Normal  Starting                 5h4m                 kubelet          Starting kubelet.
  Normal  RegisteredNode           5h4m                 node-controller  Node minikube event: Registered Node minikube in Controller
  Normal  Starting                 8m9s                 kubelet          Starting kubelet.
  Normal  NodeHasSufficientMemory  8m9s (x8 over 8m9s)  kubelet          Node minikube status is now: NodeHasSufficientMemory
  Normal  NodeHasNoDiskPressure    8m9s (x8 over 8m9s)  kubelet          Node minikube status is now: NodeHasNoDiskPressure
  Normal  NodeHasSufficientPID     8m9s (x7 over 8m9s)  kubelet          Node minikube status is now: NodeHasSufficientPID
  Normal  NodeAllocatableEnforced  8m9s                 kubelet          Updated Node Allocatable limit across pods
  Normal  RegisteredNode           7m51s                node-controller  Node minikube event: Registered Node minikube in Controller

* 
* ==> dmesg <==
* [Aug16 15:50] ACPI: SRAT not present
[  +0.000000] KASLR disabled due to lack of seed
[  +1.033756] EINJ: EINJ table not found.
[  +0.811631] systemd-fstab-generator[117]: Ignoring "noauto" for root device
[  +0.056554] systemd[1]: systemd-journald.service: unit configures an IP firewall, but the local system does not support BPF/cgroup firewalling.
[  +0.000903] systemd[1]: (This warning is only shown for the first unit using IP firewalling.)
[ +22.448813] systemd-fstab-generator[458]: Ignoring "noauto" for root device
[  +0.082904] systemd-fstab-generator[469]: Ignoring "noauto" for root device
[Aug16 15:51] systemd-fstab-generator[721]: Ignoring "noauto" for root device
[  +0.196583] systemd-fstab-generator[758]: Ignoring "noauto" for root device
[  +0.089435] systemd-fstab-generator[769]: Ignoring "noauto" for root device
[  +0.088895] systemd-fstab-generator[782]: Ignoring "noauto" for root device
[  +1.289741] systemd-fstab-generator[939]: Ignoring "noauto" for root device
[  +0.085564] systemd-fstab-generator[950]: Ignoring "noauto" for root device
[  +0.081093] systemd-fstab-generator[961]: Ignoring "noauto" for root device
[  +0.084819] systemd-fstab-generator[972]: Ignoring "noauto" for root device
[  +0.085808] systemd-fstab-generator[985]: Ignoring "noauto" for root device
[ +11.100292] systemd-fstab-generator[1205]: Ignoring "noauto" for root device
[  +0.254262] kauditd_printk_skb: 67 callbacks suppressed
[  +5.720511] kauditd_printk_skb: 2 callbacks suppressed
[  +5.594682] kauditd_printk_skb: 14 callbacks suppressed
[  +6.168871] kauditd_printk_skb: 10 callbacks suppressed

* 
* ==> etcd [02b54c8b20dc] <==
* {"level":"warn","ts":"2023-08-16T15:51:16.500Z","caller":"flags/flag.go:93","msg":"unrecognized environment variable","environment-variable":"ETCD_UNSUPPORTED_ARCH=arm64"}
{"level":"info","ts":"2023-08-16T15:51:16.502Z","caller":"etcdmain/etcd.go:73","msg":"Running: ","args":["etcd","--advertise-client-urls=https://192.168.105.2:2379","--cert-file=/var/lib/minikube/certs/etcd/server.crt","--client-cert-auth=true","--data-dir=/var/lib/minikube/etcd","--experimental-initial-corrupt-check=true","--experimental-watch-progress-notify-interval=5s","--initial-advertise-peer-urls=https://192.168.105.2:2380","--initial-cluster=minikube=https://192.168.105.2:2380","--key-file=/var/lib/minikube/certs/etcd/server.key","--listen-client-urls=https://127.0.0.1:2379,https://192.168.105.2:2379","--listen-metrics-urls=http://127.0.0.1:2381","--listen-peer-urls=https://192.168.105.2:2380","--name=minikube","--peer-cert-file=/var/lib/minikube/certs/etcd/peer.crt","--peer-client-cert-auth=true","--peer-key-file=/var/lib/minikube/certs/etcd/peer.key","--peer-trusted-ca-file=/var/lib/minikube/certs/etcd/ca.crt","--proxy-refresh-interval=70000","--snapshot-count=10000","--trusted-ca-file=/var/lib/minikube/certs/etcd/ca.crt"]}
{"level":"info","ts":"2023-08-16T15:51:16.502Z","caller":"etcdmain/etcd.go:116","msg":"server has been already initialized","data-dir":"/var/lib/minikube/etcd","dir-type":"member"}
{"level":"info","ts":"2023-08-16T15:51:16.502Z","caller":"embed/etcd.go:124","msg":"configuring peer listeners","listen-peer-urls":["https://192.168.105.2:2380"]}
{"level":"info","ts":"2023-08-16T15:51:16.502Z","caller":"embed/etcd.go:484","msg":"starting with peer TLS","tls-info":"cert = /var/lib/minikube/certs/etcd/peer.crt, key = /var/lib/minikube/certs/etcd/peer.key, client-cert=, client-key=, trusted-ca = /var/lib/minikube/certs/etcd/ca.crt, client-cert-auth = true, crl-file = ","cipher-suites":[]}
{"level":"info","ts":"2023-08-16T15:51:16.503Z","caller":"embed/etcd.go:132","msg":"configuring client listeners","listen-client-urls":["https://127.0.0.1:2379","https://192.168.105.2:2379"]}
{"level":"info","ts":"2023-08-16T15:51:16.504Z","caller":"embed/etcd.go:306","msg":"starting an etcd server","etcd-version":"3.5.7","git-sha":"215b53cf3","go-version":"go1.17.13","go-os":"linux","go-arch":"arm64","max-cpu-set":2,"max-cpu-available":2,"member-initialized":true,"name":"minikube","data-dir":"/var/lib/minikube/etcd","wal-dir":"","wal-dir-dedicated":"","member-dir":"/var/lib/minikube/etcd/member","force-new-cluster":false,"heartbeat-interval":"100ms","election-timeout":"1s","initial-election-tick-advance":true,"snapshot-count":10000,"max-wals":5,"max-snapshots":5,"snapshot-catchup-entries":5000,"initial-advertise-peer-urls":["https://192.168.105.2:2380"],"listen-peer-urls":["https://192.168.105.2:2380"],"advertise-client-urls":["https://192.168.105.2:2379"],"listen-client-urls":["https://127.0.0.1:2379","https://192.168.105.2:2379"],"listen-metrics-urls":["http://127.0.0.1:2381"],"cors":["*"],"host-whitelist":["*"],"initial-cluster":"","initial-cluster-state":"new","initial-cluster-token":"","quota-backend-bytes":2147483648,"max-request-bytes":1572864,"max-concurrent-streams":4294967295,"pre-vote":true,"initial-corrupt-check":true,"corrupt-check-time-interval":"0s","compact-check-time-enabled":false,"compact-check-time-interval":"1m0s","auto-compaction-mode":"periodic","auto-compaction-retention":"0s","auto-compaction-interval":"0s","discovery-url":"","discovery-proxy":"","downgrade-check-interval":"5s"}
{"level":"info","ts":"2023-08-16T15:51:16.519Z","caller":"etcdserver/backend.go:81","msg":"opened backend db","path":"/var/lib/minikube/etcd/member/snap/db","took":"14.125917ms"}
{"level":"info","ts":"2023-08-16T15:51:17.611Z","caller":"etcdserver/server.go:509","msg":"recovered v2 store from snapshot","snapshot-index":260026,"snapshot-size":"7.5 kB"}
{"level":"info","ts":"2023-08-16T15:51:17.611Z","caller":"etcdserver/server.go:522","msg":"recovered v3 backend from snapshot","backend-size-bytes":2338816,"backend-size":"2.3 MB","backend-size-in-use-bytes":1392640,"backend-size-in-use":"1.4 MB"}
{"level":"info","ts":"2023-08-16T15:51:17.681Z","caller":"etcdserver/raft.go:529","msg":"restarting local member","cluster-id":"6e03e7863b4f9c54","local-member-id":"c46d288d2fcb0590","commit-index":260620}
{"level":"info","ts":"2023-08-16T15:51:17.681Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"c46d288d2fcb0590 switched to configuration voters=(14154013790752671120)"}
{"level":"info","ts":"2023-08-16T15:51:17.681Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"c46d288d2fcb0590 became follower at term 3"}
{"level":"info","ts":"2023-08-16T15:51:17.681Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"newRaft c46d288d2fcb0590 [peers: [c46d288d2fcb0590], term: 3, commit: 260620, applied: 260026, lastindex: 260620, lastterm: 3]"}
{"level":"info","ts":"2023-08-16T15:51:17.681Z","caller":"api/capability.go:75","msg":"enabled capabilities for version","cluster-version":"3.5"}
{"level":"info","ts":"2023-08-16T15:51:17.681Z","caller":"membership/cluster.go:278","msg":"recovered/added member from store","cluster-id":"6e03e7863b4f9c54","local-member-id":"c46d288d2fcb0590","recovered-remote-peer-id":"c46d288d2fcb0590","recovered-remote-peer-urls":["https://192.168.105.2:2380"]}
{"level":"info","ts":"2023-08-16T15:51:17.681Z","caller":"membership/cluster.go:287","msg":"set cluster version from store","cluster-version":"3.5"}
{"level":"warn","ts":"2023-08-16T15:51:17.682Z","caller":"auth/store.go:1234","msg":"simple token is not cryptographically signed"}
{"level":"info","ts":"2023-08-16T15:51:17.682Z","caller":"mvcc/kvstore.go:323","msg":"restored last compact revision","meta-bucket-name":"meta","meta-bucket-name-key":"finishedCompactRev","restored-compact-revision":207445}
{"level":"info","ts":"2023-08-16T15:51:17.684Z","caller":"mvcc/kvstore.go:393","msg":"kvstore restored","current-rev":207812}
{"level":"info","ts":"2023-08-16T15:51:17.684Z","caller":"etcdserver/quota.go:94","msg":"enabled backend quota with default value","quota-name":"v3-applier","quota-size-bytes":2147483648,"quota-size":"2.1 GB"}
{"level":"info","ts":"2023-08-16T15:51:17.685Z","caller":"etcdserver/corrupt.go:95","msg":"starting initial corruption check","local-member-id":"c46d288d2fcb0590","timeout":"7s"}
{"level":"info","ts":"2023-08-16T15:51:17.685Z","caller":"etcdserver/corrupt.go:165","msg":"initial corruption checking passed; no corruption","local-member-id":"c46d288d2fcb0590"}
{"level":"info","ts":"2023-08-16T15:51:17.685Z","caller":"etcdserver/server.go:845","msg":"starting etcd server","local-member-id":"c46d288d2fcb0590","local-server-version":"3.5.7","cluster-id":"6e03e7863b4f9c54","cluster-version":"3.5"}
{"level":"info","ts":"2023-08-16T15:51:17.685Z","caller":"etcdserver/server.go:738","msg":"started as single-node; fast-forwarding election ticks","local-member-id":"c46d288d2fcb0590","forward-ticks":9,"forward-duration":"900ms","election-ticks":10,"election-timeout":"1s"}
{"level":"info","ts":"2023-08-16T15:51:17.685Z","caller":"fileutil/purge.go:44","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/snap","suffix":"snap.db","max":5,"interval":"30s"}
{"level":"info","ts":"2023-08-16T15:51:17.685Z","caller":"fileutil/purge.go:44","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/snap","suffix":"snap","max":5,"interval":"30s"}
{"level":"info","ts":"2023-08-16T15:51:17.685Z","caller":"fileutil/purge.go:44","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/wal","suffix":"wal","max":5,"interval":"30s"}
{"level":"info","ts":"2023-08-16T15:51:17.686Z","caller":"embed/etcd.go:687","msg":"starting with client TLS","tls-info":"cert = /var/lib/minikube/certs/etcd/server.crt, key = /var/lib/minikube/certs/etcd/server.key, client-cert=, client-key=, trusted-ca = /var/lib/minikube/certs/etcd/ca.crt, client-cert-auth = true, crl-file = ","cipher-suites":[]}
{"level":"info","ts":"2023-08-16T15:51:17.686Z","caller":"embed/etcd.go:275","msg":"now serving peer/client/metrics","local-member-id":"c46d288d2fcb0590","initial-advertise-peer-urls":["https://192.168.105.2:2380"],"listen-peer-urls":["https://192.168.105.2:2380"],"advertise-client-urls":["https://192.168.105.2:2379"],"listen-client-urls":["https://127.0.0.1:2379","https://192.168.105.2:2379"],"listen-metrics-urls":["http://127.0.0.1:2381"]}
{"level":"info","ts":"2023-08-16T15:51:17.686Z","caller":"embed/etcd.go:762","msg":"serving metrics","address":"http://127.0.0.1:2381"}
{"level":"info","ts":"2023-08-16T15:51:17.687Z","caller":"embed/etcd.go:586","msg":"serving peer traffic","address":"192.168.105.2:2380"}
{"level":"info","ts":"2023-08-16T15:51:17.687Z","caller":"embed/etcd.go:558","msg":"cmux::serve","address":"192.168.105.2:2380"}
{"level":"info","ts":"2023-08-16T15:51:18.083Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"c46d288d2fcb0590 is starting a new election at term 3"}
{"level":"info","ts":"2023-08-16T15:51:18.083Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"c46d288d2fcb0590 became pre-candidate at term 3"}
{"level":"info","ts":"2023-08-16T15:51:18.083Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"c46d288d2fcb0590 received MsgPreVoteResp from c46d288d2fcb0590 at term 3"}
{"level":"info","ts":"2023-08-16T15:51:18.083Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"c46d288d2fcb0590 became candidate at term 4"}
{"level":"info","ts":"2023-08-16T15:51:18.083Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"c46d288d2fcb0590 received MsgVoteResp from c46d288d2fcb0590 at term 4"}
{"level":"info","ts":"2023-08-16T15:51:18.083Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"c46d288d2fcb0590 became leader at term 4"}
{"level":"info","ts":"2023-08-16T15:51:18.083Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"raft.node: c46d288d2fcb0590 elected leader c46d288d2fcb0590 at term 4"}
{"level":"info","ts":"2023-08-16T15:51:18.087Z","caller":"etcdserver/server.go:2062","msg":"published local member to cluster through raft","local-member-id":"c46d288d2fcb0590","local-member-attributes":"{Name:minikube ClientURLs:[https://192.168.105.2:2379]}","request-path":"/0/members/c46d288d2fcb0590/attributes","cluster-id":"6e03e7863b4f9c54","publish-timeout":"7s"}
{"level":"info","ts":"2023-08-16T15:51:18.087Z","caller":"embed/serve.go:100","msg":"ready to serve client requests"}
{"level":"info","ts":"2023-08-16T15:51:18.088Z","caller":"embed/serve.go:100","msg":"ready to serve client requests"}
{"level":"info","ts":"2023-08-16T15:51:18.088Z","caller":"embed/serve.go:198","msg":"serving client traffic securely","address":"127.0.0.1:2379"}
{"level":"info","ts":"2023-08-16T15:51:18.089Z","caller":"etcdmain/main.go:44","msg":"notifying init daemon"}
{"level":"info","ts":"2023-08-16T15:51:18.089Z","caller":"etcdmain/main.go:50","msg":"successfully notified init daemon"}
{"level":"info","ts":"2023-08-16T15:51:18.089Z","caller":"embed/serve.go:198","msg":"serving client traffic securely","address":"192.168.105.2:2379"}

* 
* ==> etcd [5a9deb8a0f84] <==
* {"level":"info","ts":"2023-08-16T10:54:29.146Z","caller":"embed/etcd.go:124","msg":"configuring peer listeners","listen-peer-urls":["https://192.168.105.2:2380"]}
{"level":"info","ts":"2023-08-16T10:54:29.146Z","caller":"embed/etcd.go:484","msg":"starting with peer TLS","tls-info":"cert = /var/lib/minikube/certs/etcd/peer.crt, key = /var/lib/minikube/certs/etcd/peer.key, client-cert=, client-key=, trusted-ca = /var/lib/minikube/certs/etcd/ca.crt, client-cert-auth = true, crl-file = ","cipher-suites":[]}
{"level":"info","ts":"2023-08-16T10:54:29.146Z","caller":"embed/etcd.go:132","msg":"configuring client listeners","listen-client-urls":["https://127.0.0.1:2379","https://192.168.105.2:2379"]}
{"level":"info","ts":"2023-08-16T10:54:29.147Z","caller":"embed/etcd.go:306","msg":"starting an etcd server","etcd-version":"3.5.7","git-sha":"215b53cf3","go-version":"go1.17.13","go-os":"linux","go-arch":"arm64","max-cpu-set":2,"max-cpu-available":2,"member-initialized":true,"name":"minikube","data-dir":"/var/lib/minikube/etcd","wal-dir":"","wal-dir-dedicated":"","member-dir":"/var/lib/minikube/etcd/member","force-new-cluster":false,"heartbeat-interval":"100ms","election-timeout":"1s","initial-election-tick-advance":true,"snapshot-count":10000,"max-wals":5,"max-snapshots":5,"snapshot-catchup-entries":5000,"initial-advertise-peer-urls":["https://192.168.105.2:2380"],"listen-peer-urls":["https://192.168.105.2:2380"],"advertise-client-urls":["https://192.168.105.2:2379"],"listen-client-urls":["https://127.0.0.1:2379","https://192.168.105.2:2379"],"listen-metrics-urls":["http://127.0.0.1:2381"],"cors":["*"],"host-whitelist":["*"],"initial-cluster":"","initial-cluster-state":"new","initial-cluster-token":"","quota-backend-bytes":2147483648,"max-request-bytes":1572864,"max-concurrent-streams":4294967295,"pre-vote":true,"initial-corrupt-check":true,"corrupt-check-time-interval":"0s","compact-check-time-enabled":false,"compact-check-time-interval":"1m0s","auto-compaction-mode":"periodic","auto-compaction-retention":"0s","auto-compaction-interval":"0s","discovery-url":"","discovery-proxy":"","downgrade-check-interval":"5s"}
{"level":"info","ts":"2023-08-16T10:54:29.191Z","caller":"etcdserver/backend.go:81","msg":"opened backend db","path":"/var/lib/minikube/etcd/member/snap/db","took":"44.182497ms"}
{"level":"info","ts":"2023-08-16T10:54:30.283Z","caller":"etcdserver/server.go:509","msg":"recovered v2 store from snapshot","snapshot-index":250025,"snapshot-size":"7.1 kB"}
{"level":"info","ts":"2023-08-16T10:54:30.283Z","caller":"etcdserver/server.go:522","msg":"recovered v3 backend from snapshot","backend-size-bytes":2228224,"backend-size":"2.2 MB","backend-size-in-use-bytes":970752,"backend-size-in-use":"971 kB"}
{"level":"info","ts":"2023-08-16T10:54:30.355Z","caller":"etcdserver/raft.go:529","msg":"restarting local member","cluster-id":"6e03e7863b4f9c54","local-member-id":"c46d288d2fcb0590","commit-index":258833}
{"level":"info","ts":"2023-08-16T10:54:30.355Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"c46d288d2fcb0590 switched to configuration voters=(14154013790752671120)"}
{"level":"info","ts":"2023-08-16T10:54:30.355Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"c46d288d2fcb0590 became follower at term 2"}
{"level":"info","ts":"2023-08-16T10:54:30.355Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"newRaft c46d288d2fcb0590 [peers: [c46d288d2fcb0590], term: 2, commit: 258833, applied: 250025, lastindex: 258833, lastterm: 2]"}
{"level":"info","ts":"2023-08-16T10:54:30.355Z","caller":"api/capability.go:75","msg":"enabled capabilities for version","cluster-version":"3.5"}
{"level":"info","ts":"2023-08-16T10:54:30.355Z","caller":"membership/cluster.go:278","msg":"recovered/added member from store","cluster-id":"6e03e7863b4f9c54","local-member-id":"c46d288d2fcb0590","recovered-remote-peer-id":"c46d288d2fcb0590","recovered-remote-peer-urls":["https://192.168.105.2:2380"]}
{"level":"info","ts":"2023-08-16T10:54:30.355Z","caller":"membership/cluster.go:287","msg":"set cluster version from store","cluster-version":"3.5"}
{"level":"warn","ts":"2023-08-16T10:54:30.356Z","caller":"auth/store.go:1234","msg":"simple token is not cryptographically signed"}
{"level":"info","ts":"2023-08-16T10:54:30.356Z","caller":"mvcc/kvstore.go:323","msg":"restored last compact revision","meta-bucket-name":"meta","meta-bucket-name-key":"finishedCompactRev","restored-compact-revision":205933}
{"level":"info","ts":"2023-08-16T10:54:30.357Z","caller":"mvcc/kvstore.go:393","msg":"kvstore restored","current-rev":206377}
{"level":"info","ts":"2023-08-16T10:54:30.358Z","caller":"etcdserver/quota.go:94","msg":"enabled backend quota with default value","quota-name":"v3-applier","quota-size-bytes":2147483648,"quota-size":"2.1 GB"}
{"level":"info","ts":"2023-08-16T10:54:30.359Z","caller":"etcdserver/corrupt.go:95","msg":"starting initial corruption check","local-member-id":"c46d288d2fcb0590","timeout":"7s"}
{"level":"info","ts":"2023-08-16T10:54:30.359Z","caller":"etcdserver/corrupt.go:165","msg":"initial corruption checking passed; no corruption","local-member-id":"c46d288d2fcb0590"}
{"level":"info","ts":"2023-08-16T10:54:30.359Z","caller":"etcdserver/server.go:845","msg":"starting etcd server","local-member-id":"c46d288d2fcb0590","local-server-version":"3.5.7","cluster-id":"6e03e7863b4f9c54","cluster-version":"3.5"}
{"level":"info","ts":"2023-08-16T10:54:30.362Z","caller":"embed/etcd.go:687","msg":"starting with client TLS","tls-info":"cert = /var/lib/minikube/certs/etcd/server.crt, key = /var/lib/minikube/certs/etcd/server.key, client-cert=, client-key=, trusted-ca = /var/lib/minikube/certs/etcd/ca.crt, client-cert-auth = true, crl-file = ","cipher-suites":[]}
{"level":"info","ts":"2023-08-16T10:54:30.363Z","caller":"embed/etcd.go:275","msg":"now serving peer/client/metrics","local-member-id":"c46d288d2fcb0590","initial-advertise-peer-urls":["https://192.168.105.2:2380"],"listen-peer-urls":["https://192.168.105.2:2380"],"advertise-client-urls":["https://192.168.105.2:2379"],"listen-client-urls":["https://127.0.0.1:2379","https://192.168.105.2:2379"],"listen-metrics-urls":["http://127.0.0.1:2381"]}
{"level":"info","ts":"2023-08-16T10:54:30.363Z","caller":"embed/etcd.go:762","msg":"serving metrics","address":"http://127.0.0.1:2381"}
{"level":"info","ts":"2023-08-16T10:54:30.363Z","caller":"etcdserver/server.go:738","msg":"started as single-node; fast-forwarding election ticks","local-member-id":"c46d288d2fcb0590","forward-ticks":9,"forward-duration":"900ms","election-ticks":10,"election-timeout":"1s"}
{"level":"info","ts":"2023-08-16T10:54:30.363Z","caller":"fileutil/purge.go:44","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/snap","suffix":"snap.db","max":5,"interval":"30s"}
{"level":"info","ts":"2023-08-16T10:54:30.363Z","caller":"fileutil/purge.go:44","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/snap","suffix":"snap","max":5,"interval":"30s"}
{"level":"info","ts":"2023-08-16T10:54:30.363Z","caller":"fileutil/purge.go:44","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/wal","suffix":"wal","max":5,"interval":"30s"}
{"level":"info","ts":"2023-08-16T10:54:30.364Z","caller":"embed/etcd.go:586","msg":"serving peer traffic","address":"192.168.105.2:2380"}
{"level":"info","ts":"2023-08-16T10:54:30.364Z","caller":"embed/etcd.go:558","msg":"cmux::serve","address":"192.168.105.2:2380"}
{"level":"info","ts":"2023-08-16T10:54:30.957Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"c46d288d2fcb0590 is starting a new election at term 2"}
{"level":"info","ts":"2023-08-16T10:54:30.958Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"c46d288d2fcb0590 became pre-candidate at term 2"}
{"level":"info","ts":"2023-08-16T10:54:30.958Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"c46d288d2fcb0590 received MsgPreVoteResp from c46d288d2fcb0590 at term 2"}
{"level":"info","ts":"2023-08-16T10:54:30.958Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"c46d288d2fcb0590 became candidate at term 3"}
{"level":"info","ts":"2023-08-16T10:54:30.958Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"c46d288d2fcb0590 received MsgVoteResp from c46d288d2fcb0590 at term 3"}
{"level":"info","ts":"2023-08-16T10:54:30.958Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"c46d288d2fcb0590 became leader at term 3"}
{"level":"info","ts":"2023-08-16T10:54:30.958Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"raft.node: c46d288d2fcb0590 elected leader c46d288d2fcb0590 at term 3"}
{"level":"info","ts":"2023-08-16T10:54:30.963Z","caller":"embed/serve.go:100","msg":"ready to serve client requests"}
{"level":"info","ts":"2023-08-16T10:54:30.963Z","caller":"embed/serve.go:100","msg":"ready to serve client requests"}
{"level":"info","ts":"2023-08-16T10:54:30.968Z","caller":"embed/serve.go:198","msg":"serving client traffic securely","address":"192.168.105.2:2379"}
{"level":"info","ts":"2023-08-16T10:54:30.963Z","caller":"etcdserver/server.go:2062","msg":"published local member to cluster through raft","local-member-id":"c46d288d2fcb0590","local-member-attributes":"{Name:minikube ClientURLs:[https://192.168.105.2:2379]}","request-path":"/0/members/c46d288d2fcb0590/attributes","cluster-id":"6e03e7863b4f9c54","publish-timeout":"7s"}
{"level":"info","ts":"2023-08-16T10:54:30.969Z","caller":"embed/serve.go:198","msg":"serving client traffic securely","address":"127.0.0.1:2379"}
{"level":"info","ts":"2023-08-16T10:54:30.971Z","caller":"etcdmain/main.go:44","msg":"notifying init daemon"}
{"level":"info","ts":"2023-08-16T10:54:30.971Z","caller":"etcdmain/main.go:50","msg":"successfully notified init daemon"}
{"level":"info","ts":"2023-08-16T11:04:30.999Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":206727}
{"level":"info","ts":"2023-08-16T11:04:31.022Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":206727,"took":"19.808567ms","hash":2640530791}
{"level":"info","ts":"2023-08-16T11:04:31.023Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":2640530791,"revision":206727,"compact-revision":205933}
{"level":"info","ts":"2023-08-16T11:09:31.004Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":206966}
{"level":"info","ts":"2023-08-16T11:09:31.006Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":206966,"took":"1.918108ms","hash":2426240128}
{"level":"info","ts":"2023-08-16T11:09:31.006Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":2426240128,"revision":206966,"compact-revision":206727}
{"level":"info","ts":"2023-08-16T11:12:16.592Z","caller":"etcdserver/server.go:1395","msg":"triggering snapshot","local-member-id":"c46d288d2fcb0590","local-member-applied-index":260026,"local-member-snapshot-index":250025,"local-member-snapshot-count":10000}
{"level":"info","ts":"2023-08-16T11:12:16.597Z","caller":"etcdserver/server.go:2413","msg":"saved snapshot","snapshot-index":260026}
{"level":"info","ts":"2023-08-16T11:12:16.597Z","caller":"etcdserver/server.go:2443","msg":"compacted Raft logs","compact-index":255026}
{"level":"info","ts":"2023-08-16T11:12:30.411Z","caller":"fileutil/purge.go:85","msg":"purged","path":"/var/lib/minikube/etcd/member/snap/0000000000000002-0000000000033465.snap"}
{"level":"info","ts":"2023-08-16T11:14:31.013Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":207206}
{"level":"info","ts":"2023-08-16T11:14:31.016Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":207206,"took":"1.935037ms","hash":1948261178}
{"level":"info","ts":"2023-08-16T11:14:31.017Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":1948261178,"revision":207206,"compact-revision":206966}
{"level":"info","ts":"2023-08-16T11:19:31.023Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":207445}
{"level":"info","ts":"2023-08-16T11:19:31.027Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":207445,"took":"2.923078ms","hash":1479496573}
{"level":"info","ts":"2023-08-16T11:19:31.027Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":1479496573,"revision":207445,"compact-revision":207206}

* 
* ==> kernel <==
*  15:59:23 up 8 min,  0 users,  load average: 0.67, 0.55, 0.32
Linux minikube 5.10.57 #1 SMP PREEMPT Fri Jul 14 22:49:12 UTC 2023 aarch64 GNU/Linux
PRETTY_NAME="Buildroot 2021.02.12"

* 
* ==> kube-apiserver [32d50646391c] <==
* I0816 10:54:31.992374       1 cache.go:32] Waiting for caches to sync for autoregister controller
I0816 10:54:31.992378       1 cache.go:39] Caches are synced for autoregister controller
I0816 10:54:32.019294       1 shared_informer.go:318] Caches are synced for cluster_authentication_trust_controller
I0816 10:54:32.019516       1 apf_controller.go:366] Running API Priority and Fairness config worker
I0816 10:54:32.019523       1 apf_controller.go:369] Running API Priority and Fairness periodic rebalancing process
I0816 10:54:32.023679       1 cache.go:39] Caches are synced for AvailableConditionController controller
I0816 10:54:32.647550       1 controller.go:132] OpenAPI AggregationController: action for item k8s_internal_local_delegation_chain_0000000000: Nothing (removed from the queue).
I0816 10:54:32.824344       1 storage_scheduling.go:111] all system priority classes are created successfully or already exist.
I0816 10:54:33.104620       1 controller.go:624] quota admission added evaluator for: serviceaccounts
I0816 10:54:33.109850       1 controller.go:624] quota admission added evaluator for: deployments.apps
I0816 10:54:33.140921       1 controller.go:624] quota admission added evaluator for: daemonsets.apps
I0816 10:54:33.157420       1 controller.go:624] quota admission added evaluator for: roles.rbac.authorization.k8s.io
I0816 10:54:33.161159       1 controller.go:624] quota admission added evaluator for: rolebindings.rbac.authorization.k8s.io
I0816 10:54:36.965036       1 handler_discovery.go:325] DiscoveryManager: Failed to download discovery for kube-system/metrics-server:443: 503 request timed out
I0816 10:54:36.965125       1 handler.go:232] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
E0816 10:54:37.029291       1 available_controller.go:460] v1beta1.metrics.k8s.io failed with: failing or missing response from https://10.106.167.239:443/apis/metrics.k8s.io/v1beta1: Get "https://10.106.167.239:443/apis/metrics.k8s.io/v1beta1": dial tcp 10.106.167.239:443: i/o timeout
E0816 10:54:40.739784       1 storage.go:470] Address {10.244.0.4  0x4003e26f40 0x4003fb7b90} isn't valid (pod ip(s) doesn't match endpoint ip, skipping: [] vs 10.244.0.4 (kubernetes-dashboard/dashboard-metrics-scraper-5dd9cbfd69-nfdf8))
E0816 10:54:40.740143       1 storage.go:480] Failed to find a valid address, skipping subset: &{[{10.244.0.4  0x4003e26f40 0x4003fb7b90}] [] [{ 8000 TCP <nil>}]}
E0816 10:54:42.030439       1 available_controller.go:460] v1beta1.metrics.k8s.io failed with: failing or missing response from https://10.106.167.239:443/apis/metrics.k8s.io/v1beta1: Get "https://10.106.167.239:443/apis/metrics.k8s.io/v1beta1": dial tcp 10.106.167.239:443: i/o timeout
I0816 10:54:43.909678       1 controller.go:624] quota admission added evaluator for: endpoints
I0816 10:54:43.910050       1 controller.go:624] quota admission added evaluator for: endpoints
I0816 10:54:44.046691       1 controller.go:624] quota admission added evaluator for: endpointslices.discovery.k8s.io
E0816 10:54:45.117716       1 available_controller.go:460] v1beta1.metrics.k8s.io failed with: failing or missing response from https://10.106.167.239:443/apis/metrics.k8s.io/v1beta1: Get "https://10.106.167.239:443/apis/metrics.k8s.io/v1beta1": dial tcp 10.106.167.239:443: connect: no route to host
W0816 10:54:48.284270       1 handler_proxy.go:100] no RequestInfo found in the context
E0816 10:54:48.285817       1 controller.go:113] loading OpenAPI spec for "v1beta1.metrics.k8s.io" failed with: failed to retrieve openAPI spec, http error: ResponseCode: 503, Body: service unavailable
, Header: map[Content-Type:[text/plain; charset=utf-8] X-Content-Type-Options:[nosniff]]
I0816 10:54:48.285830       1 controller.go:126] OpenAPI AggregationController: action for item v1beta1.metrics.k8s.io: Rate Limited Requeue.
I0816 10:54:48.304543       1 handler.go:232] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
I0816 10:55:31.840565       1 handler.go:232] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
I0816 10:56:31.838877       1 handler.go:232] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
I0816 10:57:31.842989       1 handler.go:232] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
I0816 10:58:31.854590       1 handler.go:232] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
I0816 10:59:31.839380       1 handler.go:232] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
I0816 10:59:31.956661       1 handler.go:232] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
I0816 11:00:31.836020       1 handler.go:232] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
I0816 11:01:31.837068       1 handler.go:232] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
I0816 11:02:31.831369       1 handler.go:232] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
I0816 11:03:31.832496       1 handler.go:232] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
I0816 11:04:31.832591       1 handler.go:232] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
I0816 11:04:31.964022       1 handler.go:232] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
I0816 11:05:31.836957       1 handler.go:232] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
I0816 11:06:31.852569       1 handler.go:232] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
I0816 11:07:31.839045       1 handler.go:232] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
I0816 11:08:31.837946       1 handler.go:232] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
I0816 11:09:31.839478       1 handler.go:232] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
I0816 11:09:31.974776       1 handler.go:232] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
I0816 11:10:31.838508       1 handler.go:232] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
I0816 11:11:31.856461       1 handler.go:232] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
I0816 11:12:31.836550       1 handler.go:232] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
I0816 11:13:31.828880       1 handler.go:232] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
I0816 11:14:31.843029       1 handler.go:232] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
I0816 11:14:31.981012       1 handler.go:232] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
I0816 11:15:31.838832       1 handler.go:232] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
I0816 11:16:31.834940       1 handler.go:232] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
I0816 11:17:31.840340       1 handler.go:232] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
I0816 11:18:31.844157       1 handler.go:232] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
I0816 11:19:31.841907       1 handler.go:232] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
I0816 11:19:31.993260       1 handler.go:232] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
I0816 11:20:31.837445       1 handler.go:232] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
I0816 11:21:31.842691       1 handler.go:232] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager

* 
* ==> kube-apiserver [5eaa85cbd11f] <==
* I0816 15:51:18.873395       1 dynamic_cafile_content.go:157] "Starting controller" name="client-ca-bundle::/var/lib/minikube/certs/ca.crt"
I0816 15:51:18.873438       1 dynamic_cafile_content.go:157] "Starting controller" name="request-header::/var/lib/minikube/certs/front-proxy-ca.crt"
I0816 15:51:18.888863       1 controller.go:85] Starting OpenAPI controller
I0816 15:51:18.889066       1 controller.go:85] Starting OpenAPI V3 controller
I0816 15:51:18.889115       1 naming_controller.go:291] Starting NamingConditionController
I0816 15:51:18.889185       1 establishing_controller.go:76] Starting EstablishingController
I0816 15:51:18.889231       1 nonstructuralschema_controller.go:192] Starting NonStructuralSchemaConditionController
I0816 15:51:18.889262       1 apiapproval_controller.go:186] Starting KubernetesAPIApprovalPolicyConformantConditionController
I0816 15:51:18.889302       1 crd_finalizer.go:266] Starting CRDFinalizer
I0816 15:51:18.870897       1 dynamic_serving_content.go:132] "Starting controller" name="aggregator-proxy-cert::/var/lib/minikube/certs/front-proxy-client.crt::/var/lib/minikube/certs/front-proxy-client.key"
I0816 15:51:18.951143       1 controller.go:624] quota admission added evaluator for: leases.coordination.k8s.io
E0816 15:51:18.958579       1 controller.go:155] Error removing old endpoints from kubernetes service: no master IPs were listed in storage, refusing to erase all endpoints for the kubernetes service
I0816 15:51:18.970434       1 cache.go:39] Caches are synced for AvailableConditionController controller
I0816 15:51:18.972438       1 apf_controller.go:366] Running API Priority and Fairness config worker
I0816 15:51:18.976378       1 apf_controller.go:369] Running API Priority and Fairness periodic rebalancing process
I0816 15:51:18.973393       1 cache.go:39] Caches are synced for APIServiceRegistrationController controller
I0816 15:51:18.973432       1 shared_informer.go:318] Caches are synced for crd-autoregister
I0816 15:51:18.974172       1 shared_informer.go:318] Caches are synced for cluster_authentication_trust_controller
I0816 15:51:18.990393       1 aggregator.go:152] initial CRD sync complete...
I0816 15:51:18.990414       1 autoregister_controller.go:141] Starting autoregister controller
I0816 15:51:18.990457       1 cache.go:32] Waiting for caches to sync for autoregister controller
I0816 15:51:18.990466       1 cache.go:39] Caches are synced for autoregister controller
I0816 15:51:18.974818       1 shared_informer.go:318] Caches are synced for configmaps
I0816 15:51:19.034072       1 shared_informer.go:318] Caches are synced for node_authorizer
I0816 15:51:19.708116       1 controller.go:132] OpenAPI AggregationController: action for item k8s_internal_local_delegation_chain_0000000000: Nothing (removed from the queue).
I0816 15:51:19.875579       1 storage_scheduling.go:111] all system priority classes are created successfully or already exist.
I0816 15:51:20.539963       1 controller.go:624] quota admission added evaluator for: serviceaccounts
I0816 15:51:20.544059       1 controller.go:624] quota admission added evaluator for: deployments.apps
I0816 15:51:20.561258       1 controller.go:624] quota admission added evaluator for: daemonsets.apps
I0816 15:51:20.571167       1 controller.go:624] quota admission added evaluator for: roles.rbac.authorization.k8s.io
I0816 15:51:20.574444       1 controller.go:624] quota admission added evaluator for: rolebindings.rbac.authorization.k8s.io
E0816 15:51:23.980951       1 available_controller.go:460] v1beta1.metrics.k8s.io failed with: failing or missing response from https://10.106.167.239:443/apis/metrics.k8s.io/v1beta1: Get "https://10.106.167.239:443/apis/metrics.k8s.io/v1beta1": net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)
I0816 15:51:24.032066       1 handler_discovery.go:325] DiscoveryManager: Failed to download discovery for kube-system/metrics-server:443: 503 request timed out
I0816 15:51:24.032189       1 handler.go:232] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
E0816 15:51:28.306651       1 storage.go:470] Address {10.244.0.10  0x400e8d46c0 0x40040455e0} isn't valid (pod ip(s) doesn't match endpoint ip, skipping: [] vs 10.244.0.10 (kubernetes-dashboard/dashboard-metrics-scraper-5dd9cbfd69-nfdf8))
E0816 15:51:28.307158       1 storage.go:480] Failed to find a valid address, skipping subset: &{[{10.244.0.10  0x400e8d46c0 0x40040455e0}] [] [{ 8000 TCP <nil>}]}
E0816 15:51:28.991518       1 available_controller.go:460] v1beta1.metrics.k8s.io failed with: failing or missing response from https://10.106.167.239:443/apis/metrics.k8s.io/v1beta1: Get "https://10.106.167.239:443/apis/metrics.k8s.io/v1beta1": dial tcp 10.106.167.239:443: i/o timeout
I0816 15:51:32.025269       1 controller.go:624] quota admission added evaluator for: endpoints
E0816 15:51:32.079219       1 available_controller.go:460] v1beta1.metrics.k8s.io failed with: failing or missing response from https://10.106.167.239:443/apis/metrics.k8s.io/v1beta1: Get "https://10.106.167.239:443/apis/metrics.k8s.io/v1beta1": dial tcp 10.106.167.239:443: connect: no route to host
I0816 15:51:32.123492       1 controller.go:624] quota admission added evaluator for: endpointslices.discovery.k8s.io
E0816 15:51:35.150244       1 available_controller.go:460] v1beta1.metrics.k8s.io failed with: Operation cannot be fulfilled on apiservices.apiregistration.k8s.io "v1beta1.metrics.k8s.io": the object has been modified; please apply your changes to the latest version and try again
E0816 15:51:50.029289       1 controller.go:113] loading OpenAPI spec for "v1beta1.metrics.k8s.io" failed with: Error, could not get list of group versions for APIService
I0816 15:51:50.029344       1 controller.go:126] OpenAPI AggregationController: action for item v1beta1.metrics.k8s.io: Rate Limited Requeue.
E0816 15:51:50.029303       1 controller.go:116] loading OpenAPI spec for "v1beta1.metrics.k8s.io" failed with: failed to retrieve openAPI spec, http error: ResponseCode: 503, Body: error trying to reach service: dial tcp 10.106.167.239:443: i/o timeout
, Header: map[Content-Type:[text/plain; charset=utf-8] X-Content-Type-Options:[nosniff]]
I0816 15:51:50.036192       1 controller.go:129] OpenAPI AggregationController: action for item v1beta1.metrics.k8s.io: Rate Limited Requeue.
I0816 15:51:50.074997       1 handler.go:232] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
I0816 15:51:50.077024       1 handler.go:232] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
I0816 15:52:15.928371       1 controller.go:624] quota admission added evaluator for: replicasets.apps
I0816 15:52:15.945059       1 alloc.go:330] "allocated clusterIPs" service="default/expense-service-app" clusterIPs=map[IPv4:10.106.174.212]
I0816 15:52:18.889525       1 handler.go:232] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
I0816 15:53:18.891089       1 handler.go:232] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
I0816 15:54:18.894100       1 handler.go:232] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
I0816 15:55:18.888115       1 handler.go:232] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
I0816 15:56:04.193541       1 alloc.go:330] "allocated clusterIPs" service="default/user-app-service" clusterIPs=map[IPv4:10.110.80.34]
I0816 15:56:18.897738       1 handler.go:232] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
I0816 15:56:18.965600       1 handler.go:232] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
I0816 15:57:18.888258       1 handler.go:232] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
I0816 15:58:18.886720       1 handler.go:232] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
I0816 15:59:18.887422       1 handler.go:232] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager

* 
* ==> kube-controller-manager [31be563eb51c] <==
* I0816 10:54:43.782705       1 shared_informer.go:311] Waiting for caches to sync for daemon sets
I0816 10:54:43.783936       1 controllermanager.go:638] "Started controller" controller="statefulset"
I0816 10:54:43.783956       1 core.go:228] "Warning: configure-cloud-routes is set, but no cloud provider specified. Will not configure cloud provider routes."
I0816 10:54:43.783960       1 controllermanager.go:616] "Warning: skipping controller" controller="route"
I0816 10:54:43.785107       1 stateful_set.go:161] "Starting stateful set controller"
I0816 10:54:43.785164       1 shared_informer.go:311] Waiting for caches to sync for stateful set
I0816 10:54:43.785449       1 controllermanager.go:638] "Started controller" controller="clusterrole-aggregation"
I0816 10:54:43.785845       1 clusterroleaggregation_controller.go:189] "Starting ClusterRoleAggregator controller"
I0816 10:54:43.785853       1 shared_informer.go:311] Waiting for caches to sync for ClusterRoleAggregator
I0816 10:54:43.817621       1 shared_informer.go:311] Waiting for caches to sync for resource quota
I0816 10:54:43.838113       1 shared_informer.go:318] Caches are synced for service account
I0816 10:54:43.841648       1 shared_informer.go:318] Caches are synced for certificate-csrapproving
I0816 10:54:43.851205       1 shared_informer.go:318] Caches are synced for PV protection
I0816 10:54:43.853915       1 shared_informer.go:318] Caches are synced for cronjob
I0816 10:54:43.882982       1 shared_informer.go:318] Caches are synced for endpoint
I0816 10:54:43.888644       1 shared_informer.go:318] Caches are synced for certificate-csrsigning-legacy-unknown
I0816 10:54:43.889660       1 shared_informer.go:318] Caches are synced for TTL after finished
I0816 10:54:43.889806       1 shared_informer.go:318] Caches are synced for crt configmap
I0816 10:54:43.890010       1 shared_informer.go:318] Caches are synced for certificate-csrsigning-kubelet-serving
I0816 10:54:43.890108       1 shared_informer.go:318] Caches are synced for certificate-csrsigning-kubelet-client
I0816 10:54:43.890189       1 shared_informer.go:318] Caches are synced for certificate-csrsigning-kube-apiserver-client
I0816 10:54:43.890786       1 shared_informer.go:318] Caches are synced for ClusterRoleAggregator
I0816 10:54:43.891579       1 shared_informer.go:318] Caches are synced for bootstrap_signer
I0816 10:54:43.892333       1 shared_informer.go:311] Waiting for caches to sync for garbage collector
I0816 10:54:43.898532       1 shared_informer.go:318] Caches are synced for namespace
I0816 10:54:43.907359       1 shared_informer.go:318] Caches are synced for disruption
I0816 10:54:43.928054       1 shared_informer.go:318] Caches are synced for deployment
I0816 10:54:43.947786       1 shared_informer.go:318] Caches are synced for job
I0816 10:54:43.947842       1 shared_informer.go:318] Caches are synced for ReplicaSet
I0816 10:54:43.947868       1 shared_informer.go:318] Caches are synced for expand
I0816 10:54:43.947932       1 shared_informer.go:318] Caches are synced for endpoint_slice_mirroring
I0816 10:54:43.950111       1 shared_informer.go:318] Caches are synced for ephemeral
I0816 10:54:43.953571       1 shared_informer.go:318] Caches are synced for PVC protection
I0816 10:54:43.953709       1 shared_informer.go:318] Caches are synced for ReplicationController
I0816 10:54:43.953889       1 actual_state_of_world.go:547] "Failed to update statusUpdateNeeded field in actual state of world" err="Failed to set statusUpdateNeeded to needed true, because nodeName=\"minikube\" does not exist"
I0816 10:54:43.978396       1 shared_informer.go:318] Caches are synced for node
I0816 10:54:43.978580       1 range_allocator.go:174] "Sending events to api server"
I0816 10:54:43.978676       1 range_allocator.go:178] "Starting range CIDR allocator"
I0816 10:54:43.978723       1 shared_informer.go:311] Waiting for caches to sync for cidrallocator
I0816 10:54:43.978775       1 shared_informer.go:318] Caches are synced for cidrallocator
I0816 10:54:43.980401       1 shared_informer.go:318] Caches are synced for persistent volume
I0816 10:54:43.983374       1 shared_informer.go:318] Caches are synced for daemon sets
I0816 10:54:43.986061       1 shared_informer.go:318] Caches are synced for stateful set
I0816 10:54:43.986337       1 shared_informer.go:318] Caches are synced for GC
I0816 10:54:44.014070       1 shared_informer.go:318] Caches are synced for taint
I0816 10:54:44.014949       1 taint_manager.go:206] "Starting NoExecuteTaintManager"
I0816 10:54:44.015037       1 taint_manager.go:211] "Sending events to api server"
I0816 10:54:44.015551       1 node_lifecycle_controller.go:1223] "Initializing eviction metric for zone" zone=""
I0816 10:54:44.015908       1 event.go:307] "Event occurred" object="minikube" fieldPath="" kind="Node" apiVersion="v1" type="Normal" reason="RegisteredNode" message="Node minikube event: Registered Node minikube in Controller"
I0816 10:54:44.015915       1 node_lifecycle_controller.go:875] "Missing timestamp for Node. Assuming now as a timestamp" node="minikube"
I0816 10:54:44.016816       1 node_lifecycle_controller.go:1069] "Controller detected that zone is now in new state" zone="" newState=Normal
I0816 10:54:44.018736       1 shared_informer.go:318] Caches are synced for resource quota
I0816 10:54:44.021117       1 shared_informer.go:318] Caches are synced for attach detach
I0816 10:54:44.039284       1 shared_informer.go:318] Caches are synced for endpoint_slice
I0816 10:54:44.044448       1 shared_informer.go:318] Caches are synced for TTL
I0816 10:54:44.083912       1 shared_informer.go:318] Caches are synced for resource quota
I0816 10:54:44.101681       1 shared_informer.go:318] Caches are synced for HPA
I0816 10:54:44.450649       1 shared_informer.go:318] Caches are synced for garbage collector
I0816 10:54:44.450737       1 garbagecollector.go:166] "All resource monitors have synced. Proceeding to collect garbage"
I0816 10:54:44.493168       1 shared_informer.go:318] Caches are synced for garbage collector

* 
* ==> kube-controller-manager [579d691ede2f] <==
* I0816 15:51:31.769664       1 shared_informer.go:311] Waiting for caches to sync for token_cleaner
I0816 15:51:31.769667       1 shared_informer.go:318] Caches are synced for token_cleaner
I0816 15:51:31.819130       1 controllermanager.go:638] "Started controller" controller="ephemeral-volume"
I0816 15:51:31.821126       1 controller.go:169] "Starting ephemeral volume controller"
I0816 15:51:31.821143       1 shared_informer.go:311] Waiting for caches to sync for ephemeral
I0816 15:51:31.824947       1 shared_informer.go:311] Waiting for caches to sync for resource quota
I0816 15:51:31.856457       1 shared_informer.go:318] Caches are synced for namespace
I0816 15:51:31.858849       1 shared_informer.go:311] Waiting for caches to sync for garbage collector
I0816 15:51:31.864393       1 shared_informer.go:318] Caches are synced for endpoint_slice_mirroring
I0816 15:51:31.866560       1 shared_informer.go:318] Caches are synced for ReplicaSet
I0816 15:51:31.869757       1 shared_informer.go:318] Caches are synced for PV protection
I0816 15:51:31.869773       1 shared_informer.go:318] Caches are synced for stateful set
I0816 15:51:31.870873       1 shared_informer.go:318] Caches are synced for job
I0816 15:51:31.872029       1 shared_informer.go:318] Caches are synced for certificate-csrsigning-kubelet-serving
I0816 15:51:31.872133       1 shared_informer.go:318] Caches are synced for certificate-csrsigning-kubelet-client
I0816 15:51:31.872167       1 shared_informer.go:318] Caches are synced for expand
I0816 15:51:31.873157       1 shared_informer.go:318] Caches are synced for certificate-csrsigning-kube-apiserver-client
I0816 15:51:31.873647       1 shared_informer.go:318] Caches are synced for certificate-csrsigning-legacy-unknown
I0816 15:51:31.879699       1 shared_informer.go:318] Caches are synced for certificate-csrapproving
I0816 15:51:31.906387       1 shared_informer.go:318] Caches are synced for disruption
I0816 15:51:31.916257       1 shared_informer.go:318] Caches are synced for ClusterRoleAggregator
I0816 15:51:31.920872       1 shared_informer.go:318] Caches are synced for ReplicationController
I0816 15:51:31.920877       1 shared_informer.go:318] Caches are synced for PVC protection
I0816 15:51:31.920894       1 shared_informer.go:318] Caches are synced for cronjob
I0816 15:51:31.921031       1 shared_informer.go:318] Caches are synced for TTL after finished
I0816 15:51:31.920884       1 shared_informer.go:318] Caches are synced for crt configmap
I0816 15:51:31.921168       1 shared_informer.go:318] Caches are synced for ephemeral
I0816 15:51:31.920889       1 shared_informer.go:318] Caches are synced for service account
I0816 15:51:31.920892       1 shared_informer.go:318] Caches are synced for HPA
I0816 15:51:31.922372       1 shared_informer.go:318] Caches are synced for deployment
I0816 15:51:31.933102       1 shared_informer.go:318] Caches are synced for endpoint
I0816 15:51:31.939189       1 shared_informer.go:318] Caches are synced for bootstrap_signer
I0816 15:51:31.974993       1 actual_state_of_world.go:547] "Failed to update statusUpdateNeeded field in actual state of world" err="Failed to set statusUpdateNeeded to needed true, because nodeName=\"minikube\" does not exist"
I0816 15:51:31.993121       1 shared_informer.go:318] Caches are synced for resource quota
I0816 15:51:32.023667       1 shared_informer.go:318] Caches are synced for node
I0816 15:51:32.023711       1 range_allocator.go:174] "Sending events to api server"
I0816 15:51:32.023874       1 range_allocator.go:178] "Starting range CIDR allocator"
I0816 15:51:32.023882       1 shared_informer.go:311] Waiting for caches to sync for cidrallocator
I0816 15:51:32.023885       1 shared_informer.go:318] Caches are synced for cidrallocator
I0816 15:51:32.026798       1 shared_informer.go:318] Caches are synced for resource quota
I0816 15:51:32.027603       1 shared_informer.go:318] Caches are synced for daemon sets
I0816 15:51:32.034797       1 shared_informer.go:318] Caches are synced for endpoint_slice
I0816 15:51:32.036161       1 shared_informer.go:318] Caches are synced for GC
I0816 15:51:32.036748       1 shared_informer.go:318] Caches are synced for TTL
I0816 15:51:32.061669       1 shared_informer.go:318] Caches are synced for attach detach
I0816 15:51:32.070520       1 shared_informer.go:318] Caches are synced for persistent volume
I0816 15:51:32.070976       1 shared_informer.go:318] Caches are synced for taint
I0816 15:51:32.071410       1 taint_manager.go:206] "Starting NoExecuteTaintManager"
I0816 15:51:32.071432       1 taint_manager.go:211] "Sending events to api server"
I0816 15:51:32.071757       1 node_lifecycle_controller.go:1223] "Initializing eviction metric for zone" zone=""
I0816 15:51:32.072954       1 event.go:307] "Event occurred" object="minikube" fieldPath="" kind="Node" apiVersion="v1" type="Normal" reason="RegisteredNode" message="Node minikube event: Registered Node minikube in Controller"
I0816 15:51:32.073491       1 node_lifecycle_controller.go:875] "Missing timestamp for Node. Assuming now as a timestamp" node="minikube"
I0816 15:51:32.073615       1 node_lifecycle_controller.go:1069] "Controller detected that zone is now in new state" zone="" newState=Normal
I0816 15:51:32.459141       1 shared_informer.go:318] Caches are synced for garbage collector
I0816 15:51:32.497547       1 shared_informer.go:318] Caches are synced for garbage collector
I0816 15:51:32.497582       1 garbagecollector.go:166] "All resource monitors have synced. Proceeding to collect garbage"
I0816 15:52:15.961677       1 event.go:307] "Event occurred" object="default/expense-service-app-deployment" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set expense-service-app-deployment-7dfd987bb to 1"
I0816 15:52:15.999403       1 event.go:307] "Event occurred" object="default/expense-service-app-deployment-7dfd987bb" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: expense-service-app-deployment-7dfd987bb-5t45r"
I0816 15:56:04.190122       1 event.go:307] "Event occurred" object="default/user-service-app-deployment" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set user-service-app-deployment-6869bd8cc5 to 1"
I0816 15:56:04.211697       1 event.go:307] "Event occurred" object="default/user-service-app-deployment-6869bd8cc5" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: user-service-app-deployment-6869bd8cc5-4hsdv"

* 
* ==> kube-proxy [3bf25d7c0f8e] <==
* I0816 15:51:20.362400       1 node.go:141] Successfully retrieved node IP: 192.168.105.2
I0816 15:51:20.362463       1 server_others.go:110] "Detected node IP" address="192.168.105.2"
I0816 15:51:20.362564       1 server_others.go:554] "Using iptables proxy"
I0816 15:51:20.382855       1 server_others.go:178] "kube-proxy running in single-stack mode: secondary ipFamily is not supported" ipFamily=IPv6
I0816 15:51:20.382871       1 server_others.go:192] "Using iptables Proxier"
I0816 15:51:20.383769       1 proxier.go:253] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses"
I0816 15:51:20.384330       1 server.go:658] "Version info" version="v1.27.3"
I0816 15:51:20.384380       1 server.go:660] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0816 15:51:20.387233       1 config.go:188] "Starting service config controller"
I0816 15:51:20.389012       1 shared_informer.go:311] Waiting for caches to sync for service config
I0816 15:51:20.389042       1 config.go:315] "Starting node config controller"
I0816 15:51:20.389052       1 shared_informer.go:311] Waiting for caches to sync for node config
I0816 15:51:20.389965       1 config.go:97] "Starting endpoint slice config controller"
I0816 15:51:20.389976       1 shared_informer.go:311] Waiting for caches to sync for endpoint slice config
I0816 15:51:20.489453       1 shared_informer.go:318] Caches are synced for node config
I0816 15:51:20.489484       1 shared_informer.go:318] Caches are synced for service config
I0816 15:51:20.490553       1 shared_informer.go:318] Caches are synced for endpoint slice config

* 
* ==> kube-proxy [b0f5f807e4cf] <==
* I0816 10:54:32.752810       1 node.go:141] Successfully retrieved node IP: 192.168.105.2
I0816 10:54:32.752874       1 server_others.go:110] "Detected node IP" address="192.168.105.2"
I0816 10:54:32.752911       1 server_others.go:554] "Using iptables proxy"
I0816 10:54:32.773472       1 server_others.go:178] "kube-proxy running in single-stack mode: secondary ipFamily is not supported" ipFamily=IPv6
I0816 10:54:32.773488       1 server_others.go:192] "Using iptables Proxier"
I0816 10:54:32.774330       1 proxier.go:253] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses"
I0816 10:54:32.774982       1 server.go:658] "Version info" version="v1.27.3"
I0816 10:54:32.775044       1 server.go:660] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0816 10:54:32.776463       1 config.go:188] "Starting service config controller"
I0816 10:54:32.777845       1 shared_informer.go:311] Waiting for caches to sync for service config
I0816 10:54:32.777889       1 config.go:315] "Starting node config controller"
I0816 10:54:32.777948       1 shared_informer.go:311] Waiting for caches to sync for node config
I0816 10:54:32.778618       1 config.go:97] "Starting endpoint slice config controller"
I0816 10:54:32.778660       1 shared_informer.go:311] Waiting for caches to sync for endpoint slice config
I0816 10:54:32.878293       1 shared_informer.go:318] Caches are synced for service config
I0816 10:54:32.878337       1 shared_informer.go:318] Caches are synced for node config
I0816 10:54:32.879475       1 shared_informer.go:318] Caches are synced for endpoint slice config

* 
* ==> kube-scheduler [1f3795d91eeb] <==
* I0816 10:54:29.350099       1 serving.go:348] Generated self-signed cert in-memory
W0816 10:54:29.698133       1 authentication.go:368] Error looking up in-cluster authentication configuration: Get "https://192.168.105.2:8443/api/v1/namespaces/kube-system/configmaps/extension-apiserver-authentication": dial tcp 192.168.105.2:8443: connect: connection refused
W0816 10:54:29.698352       1 authentication.go:369] Continuing without authentication configuration. This may treat all requests as anonymous.
W0816 10:54:29.698389       1 authentication.go:370] To require authentication configuration lookup to succeed, set --authentication-tolerate-lookup-failure=false
I0816 10:54:29.710428       1 server.go:154] "Starting Kubernetes Scheduler" version="v1.27.3"
I0816 10:54:29.710534       1 server.go:156] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0816 10:54:29.716158       1 secure_serving.go:210] Serving securely on 127.0.0.1:10259
I0816 10:54:29.730261       1 tlsconfig.go:240] "Starting DynamicServingCertificateController"
I0816 10:54:29.730297       1 configmap_cafile_content.go:202] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I0816 10:54:29.746501       1 shared_informer.go:311] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
W0816 10:54:31.890214       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User "system:kube-scheduler" cannot list resource "poddisruptionbudgets" in API group "policy" at the cluster scope
W0816 10:54:31.891722       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.Pod: pods is forbidden: User "system:kube-scheduler" cannot list resource "pods" in API group "" at the cluster scope
E0816 10:54:31.891828       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.PodDisruptionBudget: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User "system:kube-scheduler" cannot list resource "poddisruptionbudgets" in API group "policy" at the cluster scope
W0816 10:54:31.891412       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csinodes" in API group "storage.k8s.io" at the cluster scope
E0816 10:54:31.891924       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.CSINode: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csinodes" in API group "storage.k8s.io" at the cluster scope
W0816 10:54:31.891458       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csidrivers" in API group "storage.k8s.io" at the cluster scope
E0816 10:54:31.892001       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.CSIDriver: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csidrivers" in API group "storage.k8s.io" at the cluster scope
W0816 10:54:31.891504       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumes" in API group "" at the cluster scope
E0816 10:54:31.892116       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.PersistentVolume: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumes" in API group "" at the cluster scope
W0816 10:54:31.891578       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "storageclasses" in API group "storage.k8s.io" at the cluster scope
E0816 10:54:31.892155       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.StorageClass: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "storageclasses" in API group "storage.k8s.io" at the cluster scope
W0816 10:54:31.891648       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.Service: services is forbidden: User "system:kube-scheduler" cannot list resource "services" in API group "" at the cluster scope
E0816 10:54:31.892220       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.Service: failed to list *v1.Service: services is forbidden: User "system:kube-scheduler" cannot list resource "services" in API group "" at the cluster scope
W0816 10:54:31.891644       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumeclaims" in API group "" at the cluster scope
E0816 10:54:31.892252       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.PersistentVolumeClaim: failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumeclaims" in API group "" at the cluster scope
W0816 10:54:31.891680       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User "system:kube-scheduler" cannot list resource "replicationcontrollers" in API group "" at the cluster scope
E0816 10:54:31.892298       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.ReplicationController: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User "system:kube-scheduler" cannot list resource "replicationcontrollers" in API group "" at the cluster scope
E0816 10:54:31.891753       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.Pod: failed to list *v1.Pod: pods is forbidden: User "system:kube-scheduler" cannot list resource "pods" in API group "" at the cluster scope
W0816 10:54:31.891993       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User "system:kube-scheduler" cannot list resource "statefulsets" in API group "apps" at the cluster scope
E0816 10:54:31.892414       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.StatefulSet: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User "system:kube-scheduler" cannot list resource "statefulsets" in API group "apps" at the cluster scope
W0816 10:54:31.892422       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.Namespace: namespaces is forbidden: User "system:kube-scheduler" cannot list resource "namespaces" in API group "" at the cluster scope
E0816 10:54:31.892498       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.Namespace: failed to list *v1.Namespace: namespaces is forbidden: User "system:kube-scheduler" cannot list resource "namespaces" in API group "" at the cluster scope
W0816 10:54:31.892495       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User "system:kube-scheduler" cannot list resource "replicasets" in API group "apps" at the cluster scope
E0816 10:54:31.892567       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.ReplicaSet: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User "system:kube-scheduler" cannot list resource "replicasets" in API group "apps" at the cluster scope
W0816 10:54:31.892048       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csistoragecapacities" in API group "storage.k8s.io" at the cluster scope
E0816 10:54:31.892608       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.CSIStorageCapacity: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csistoragecapacities" in API group "storage.k8s.io" at the cluster scope
W0816 10:54:31.892348       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.Node: nodes is forbidden: User "system:kube-scheduler" cannot list resource "nodes" in API group "" at the cluster scope
E0816 10:54:31.892618       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.Node: failed to list *v1.Node: nodes is forbidden: User "system:kube-scheduler" cannot list resource "nodes" in API group "" at the cluster scope
W0816 10:54:31.922841       1 reflector.go:533] pkg/server/dynamiccertificates/configmap_cafile_content.go:206: failed to list *v1.ConfigMap: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot list resource "configmaps" in API group "" in the namespace "kube-system"
E0816 10:54:31.922870       1 reflector.go:148] pkg/server/dynamiccertificates/configmap_cafile_content.go:206: Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot list resource "configmaps" in API group "" in the namespace "kube-system"
I0816 10:54:32.947265       1 shared_informer.go:318] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file

* 
* ==> kube-scheduler [6ed612f2be8c] <==
* I0816 15:51:17.177165       1 serving.go:348] Generated self-signed cert in-memory
W0816 15:51:18.927500       1 requestheader_controller.go:193] Unable to get configmap/extension-apiserver-authentication in kube-system.  Usually fixed by 'kubectl create rolebinding -n kube-system ROLEBINDING_NAME --role=extension-apiserver-authentication-reader --serviceaccount=YOUR_NS:YOUR_SA'
W0816 15:51:18.928024       1 authentication.go:368] Error looking up in-cluster authentication configuration: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot get resource "configmaps" in API group "" in the namespace "kube-system"
W0816 15:51:18.928108       1 authentication.go:369] Continuing without authentication configuration. This may treat all requests as anonymous.
W0816 15:51:18.928158       1 authentication.go:370] To require authentication configuration lookup to succeed, set --authentication-tolerate-lookup-failure=false
I0816 15:51:18.963786       1 server.go:154] "Starting Kubernetes Scheduler" version="v1.27.3"
I0816 15:51:18.963813       1 server.go:156] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0816 15:51:18.966239       1 configmap_cafile_content.go:202] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I0816 15:51:18.966944       1 secure_serving.go:210] Serving securely on 127.0.0.1:10259
I0816 15:51:18.967052       1 tlsconfig.go:240] "Starting DynamicServingCertificateController"
I0816 15:51:18.967267       1 shared_informer.go:311] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I0816 15:51:19.067750       1 shared_informer.go:318] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file

* 
* ==> kubelet <==
* -- Journal begins at Wed 2023-08-16 15:50:37 UTC, ends at Wed 2023-08-16 15:59:23 UTC. --
Aug 16 15:51:21 minikube kubelet[1211]: E0816 15:51:21.444569    1211 nestedpendingoperations.go:348] Operation for "{volumeName:kubernetes.io/projected/a8a9d6b1-3240-46ad-b115-c8a58deca0e2-kube-api-access-bx6nt podName:a8a9d6b1-3240-46ad-b115-c8a58deca0e2 nodeName:}" failed. No retries permitted until 2023-08-16 15:51:23.44456394 +0000 UTC m=+8.955882921 (durationBeforeRetry 2s). Error: MountVolume.SetUp failed for volume "kube-api-access-bx6nt" (UniqueName: "kubernetes.io/projected/a8a9d6b1-3240-46ad-b115-c8a58deca0e2-kube-api-access-bx6nt") pod "kubernetes-dashboard-5c5cfc8747-sq86c" (UID: "a8a9d6b1-3240-46ad-b115-c8a58deca0e2") : object "kubernetes-dashboard"/"kube-root-ca.crt" not registered
Aug 16 15:51:21 minikube kubelet[1211]: E0816 15:51:21.709760    1211 pod_workers.go:1294] "Error syncing pod, skipping" err="network is not ready: container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:docker: network plugin is not ready: cni config uninitialized" pod="kube-system/coredns-5d78c9869d-ptqjm" podUID=05e0c7e4-0c61-4fa0-a77c-ce1f35365062
Aug 16 15:51:21 minikube kubelet[1211]: E0816 15:51:21.709758    1211 pod_workers.go:1294] "Error syncing pod, skipping" err="network is not ready: container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:docker: network plugin is not ready: cni config uninitialized" pod="kubernetes-dashboard/dashboard-metrics-scraper-5dd9cbfd69-nfdf8" podUID=bc574791-55c7-4d2f-8d7c-7e80aad15be7
Aug 16 15:51:21 minikube kubelet[1211]: E0816 15:51:21.709963    1211 pod_workers.go:1294] "Error syncing pod, skipping" err="network is not ready: container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:docker: network plugin is not ready: cni config uninitialized" pod="kubernetes-dashboard/kubernetes-dashboard-5c5cfc8747-sq86c" podUID=a8a9d6b1-3240-46ad-b115-c8a58deca0e2
Aug 16 15:51:21 minikube kubelet[1211]: E0816 15:51:21.710031    1211 pod_workers.go:1294] "Error syncing pod, skipping" err="network is not ready: container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:docker: network plugin is not ready: cni config uninitialized" pod="kube-system/metrics-server-844d8db974-z5wrd" podUID=4b6d6190-223d-478c-98dd-4634bd1a4cee
Aug 16 15:51:23 minikube kubelet[1211]: E0816 15:51:23.360348    1211 configmap.go:199] Couldn't get configMap kube-system/coredns: object "kube-system"/"coredns" not registered
Aug 16 15:51:23 minikube kubelet[1211]: E0816 15:51:23.360817    1211 nestedpendingoperations.go:348] Operation for "{volumeName:kubernetes.io/configmap/05e0c7e4-0c61-4fa0-a77c-ce1f35365062-config-volume podName:05e0c7e4-0c61-4fa0-a77c-ce1f35365062 nodeName:}" failed. No retries permitted until 2023-08-16 15:51:27.360805649 +0000 UTC m=+12.872124631 (durationBeforeRetry 4s). Error: MountVolume.SetUp failed for volume "config-volume" (UniqueName: "kubernetes.io/configmap/05e0c7e4-0c61-4fa0-a77c-ce1f35365062-config-volume") pod "coredns-5d78c9869d-ptqjm" (UID: "05e0c7e4-0c61-4fa0-a77c-ce1f35365062") : object "kube-system"/"coredns" not registered
Aug 16 15:51:23 minikube kubelet[1211]: E0816 15:51:23.460844    1211 projected.go:292] Couldn't get configMap kubernetes-dashboard/kube-root-ca.crt: object "kubernetes-dashboard"/"kube-root-ca.crt" not registered
Aug 16 15:51:23 minikube kubelet[1211]: E0816 15:51:23.460882    1211 projected.go:198] Error preparing data for projected volume kube-api-access-bx6nt for pod kubernetes-dashboard/kubernetes-dashboard-5c5cfc8747-sq86c: object "kubernetes-dashboard"/"kube-root-ca.crt" not registered
Aug 16 15:51:23 minikube kubelet[1211]: E0816 15:51:23.460927    1211 nestedpendingoperations.go:348] Operation for "{volumeName:kubernetes.io/projected/a8a9d6b1-3240-46ad-b115-c8a58deca0e2-kube-api-access-bx6nt podName:a8a9d6b1-3240-46ad-b115-c8a58deca0e2 nodeName:}" failed. No retries permitted until 2023-08-16 15:51:27.460916566 +0000 UTC m=+12.972235506 (durationBeforeRetry 4s). Error: MountVolume.SetUp failed for volume "kube-api-access-bx6nt" (UniqueName: "kubernetes.io/projected/a8a9d6b1-3240-46ad-b115-c8a58deca0e2-kube-api-access-bx6nt") pod "kubernetes-dashboard-5c5cfc8747-sq86c" (UID: "a8a9d6b1-3240-46ad-b115-c8a58deca0e2") : object "kubernetes-dashboard"/"kube-root-ca.crt" not registered
Aug 16 15:51:23 minikube kubelet[1211]: E0816 15:51:23.460844    1211 projected.go:292] Couldn't get configMap kubernetes-dashboard/kube-root-ca.crt: object "kubernetes-dashboard"/"kube-root-ca.crt" not registered
Aug 16 15:51:23 minikube kubelet[1211]: E0816 15:51:23.461091    1211 projected.go:198] Error preparing data for projected volume kube-api-access-kj826 for pod kubernetes-dashboard/dashboard-metrics-scraper-5dd9cbfd69-nfdf8: object "kubernetes-dashboard"/"kube-root-ca.crt" not registered
Aug 16 15:51:23 minikube kubelet[1211]: E0816 15:51:23.461135    1211 nestedpendingoperations.go:348] Operation for "{volumeName:kubernetes.io/projected/bc574791-55c7-4d2f-8d7c-7e80aad15be7-kube-api-access-kj826 podName:bc574791-55c7-4d2f-8d7c-7e80aad15be7 nodeName:}" failed. No retries permitted until 2023-08-16 15:51:27.461128691 +0000 UTC m=+12.972447672 (durationBeforeRetry 4s). Error: MountVolume.SetUp failed for volume "kube-api-access-kj826" (UniqueName: "kubernetes.io/projected/bc574791-55c7-4d2f-8d7c-7e80aad15be7-kube-api-access-kj826") pod "dashboard-metrics-scraper-5dd9cbfd69-nfdf8" (UID: "bc574791-55c7-4d2f-8d7c-7e80aad15be7") : object "kubernetes-dashboard"/"kube-root-ca.crt" not registered
Aug 16 15:51:23 minikube kubelet[1211]: E0816 15:51:23.710502    1211 pod_workers.go:1294] "Error syncing pod, skipping" err="network is not ready: container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:docker: network plugin is not ready: cni config uninitialized" pod="kube-system/metrics-server-844d8db974-z5wrd" podUID=4b6d6190-223d-478c-98dd-4634bd1a4cee
Aug 16 15:51:23 minikube kubelet[1211]: E0816 15:51:23.710807    1211 pod_workers.go:1294] "Error syncing pod, skipping" err="network is not ready: container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:docker: network plugin is not ready: cni config uninitialized" pod="kube-system/coredns-5d78c9869d-ptqjm" podUID=05e0c7e4-0c61-4fa0-a77c-ce1f35365062
Aug 16 15:51:23 minikube kubelet[1211]: E0816 15:51:23.710937    1211 pod_workers.go:1294] "Error syncing pod, skipping" err="network is not ready: container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:docker: network plugin is not ready: cni config uninitialized" pod="kubernetes-dashboard/kubernetes-dashboard-5c5cfc8747-sq86c" podUID=a8a9d6b1-3240-46ad-b115-c8a58deca0e2
Aug 16 15:51:23 minikube kubelet[1211]: E0816 15:51:23.711063    1211 pod_workers.go:1294] "Error syncing pod, skipping" err="network is not ready: container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:docker: network plugin is not ready: cni config uninitialized" pod="kubernetes-dashboard/dashboard-metrics-scraper-5dd9cbfd69-nfdf8" podUID=bc574791-55c7-4d2f-8d7c-7e80aad15be7
Aug 16 15:51:50 minikube kubelet[1211]: I0816 15:51:50.702585    1211 scope.go:115] "RemoveContainer" containerID="ffd9479bde5199caaf66280fd3b06874debd85621957e17d1ef5867328c2c5e3"
Aug 16 15:51:50 minikube kubelet[1211]: I0816 15:51:50.702913    1211 scope.go:115] "RemoveContainer" containerID="057ac53ae0884a9ce6e39145f05aedbe54e6350b9ce6ce596a214ca881c9cfb2"
Aug 16 15:51:50 minikube kubelet[1211]: E0816 15:51:50.720432    1211 pod_workers.go:1294] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"storage-provisioner\" with CrashLoopBackOff: \"back-off 10s restarting failed container=storage-provisioner pod=storage-provisioner_kube-system(46ca2804-7b31-40bc-a389-1829ebcdb03c)\"" pod="kube-system/storage-provisioner" podUID=46ca2804-7b31-40bc-a389-1829ebcdb03c
Aug 16 15:52:02 minikube kubelet[1211]: I0816 15:52:02.715352    1211 scope.go:115] "RemoveContainer" containerID="057ac53ae0884a9ce6e39145f05aedbe54e6350b9ce6ce596a214ca881c9cfb2"
Aug 16 15:52:14 minikube kubelet[1211]: E0816 15:52:14.737031    1211 iptables.go:575] "Could not set up iptables canary" err=<
Aug 16 15:52:14 minikube kubelet[1211]:         error creating chain "KUBE-KUBELET-CANARY": exit status 3: ip6tables v1.8.6 (legacy): can't initialize ip6tables table `nat': Table does not exist (do you need to insmod?)
Aug 16 15:52:14 minikube kubelet[1211]:         Perhaps ip6tables or your kernel needs to be upgraded.
Aug 16 15:52:14 minikube kubelet[1211]:  > table=nat chain=KUBE-KUBELET-CANARY
Aug 16 15:52:16 minikube kubelet[1211]: I0816 15:52:16.024181    1211 topology_manager.go:212] "Topology Admit Handler"
Aug 16 15:52:16 minikube kubelet[1211]: I0816 15:52:16.152246    1211 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-xls95\" (UniqueName: \"kubernetes.io/projected/4cb85fb9-6a62-4d06-99f8-a041d98c658a-kube-api-access-xls95\") pod \"expense-service-app-deployment-7dfd987bb-5t45r\" (UID: \"4cb85fb9-6a62-4d06-99f8-a041d98c658a\") " pod="default/expense-service-app-deployment-7dfd987bb-5t45r"
Aug 16 15:52:24 minikube kubelet[1211]: I0816 15:52:24.298946    1211 pod_startup_latency_tracker.go:102] "Observed pod startup duration" pod="default/expense-service-app-deployment-7dfd987bb-5t45r" podStartSLOduration=2.752430208 podCreationTimestamp="2023-08-16 15:52:15 +0000 UTC" firstStartedPulling="2023-08-16 15:52:16.601007336 +0000 UTC m=+62.112326318" lastFinishedPulling="2023-08-16 15:52:23.13960276 +0000 UTC m=+68.650921742" observedRunningTime="2023-08-16 15:52:24.290775259 +0000 UTC m=+69.802094324" watchObservedRunningTime="2023-08-16 15:52:24.291025632 +0000 UTC m=+69.802344655"
Aug 16 15:53:14 minikube kubelet[1211]: E0816 15:53:14.733927    1211 iptables.go:575] "Could not set up iptables canary" err=<
Aug 16 15:53:14 minikube kubelet[1211]:         error creating chain "KUBE-KUBELET-CANARY": exit status 3: ip6tables v1.8.6 (legacy): can't initialize ip6tables table `nat': Table does not exist (do you need to insmod?)
Aug 16 15:53:14 minikube kubelet[1211]:         Perhaps ip6tables or your kernel needs to be upgraded.
Aug 16 15:53:14 minikube kubelet[1211]:  > table=nat chain=KUBE-KUBELET-CANARY
Aug 16 15:54:14 minikube kubelet[1211]: E0816 15:54:14.747977    1211 iptables.go:575] "Could not set up iptables canary" err=<
Aug 16 15:54:14 minikube kubelet[1211]:         error creating chain "KUBE-KUBELET-CANARY": exit status 3: ip6tables v1.8.6 (legacy): can't initialize ip6tables table `nat': Table does not exist (do you need to insmod?)
Aug 16 15:54:14 minikube kubelet[1211]:         Perhaps ip6tables or your kernel needs to be upgraded.
Aug 16 15:54:14 minikube kubelet[1211]:  > table=nat chain=KUBE-KUBELET-CANARY
Aug 16 15:55:14 minikube kubelet[1211]: E0816 15:55:14.732907    1211 iptables.go:575] "Could not set up iptables canary" err=<
Aug 16 15:55:14 minikube kubelet[1211]:         error creating chain "KUBE-KUBELET-CANARY": exit status 3: ip6tables v1.8.6 (legacy): can't initialize ip6tables table `nat': Table does not exist (do you need to insmod?)
Aug 16 15:55:14 minikube kubelet[1211]:         Perhaps ip6tables or your kernel needs to be upgraded.
Aug 16 15:55:14 minikube kubelet[1211]:  > table=nat chain=KUBE-KUBELET-CANARY
Aug 16 15:56:04 minikube kubelet[1211]: I0816 15:56:04.223317    1211 topology_manager.go:212] "Topology Admit Handler"
Aug 16 15:56:04 minikube kubelet[1211]: I0816 15:56:04.328170    1211 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-7j2vf\" (UniqueName: \"kubernetes.io/projected/6f6daed9-2c9a-4f7b-b993-a21003655beb-kube-api-access-7j2vf\") pod \"user-service-app-deployment-6869bd8cc5-4hsdv\" (UID: \"6f6daed9-2c9a-4f7b-b993-a21003655beb\") " pod="default/user-service-app-deployment-6869bd8cc5-4hsdv"
Aug 16 15:56:12 minikube kubelet[1211]: I0816 15:56:12.607332    1211 pod_startup_latency_tracker.go:102] "Observed pod startup duration" pod="default/user-service-app-deployment-6869bd8cc5-4hsdv" podStartSLOduration=1.944293999 podCreationTimestamp="2023-08-16 15:56:04 +0000 UTC" firstStartedPulling="2023-08-16 15:56:04.801043932 +0000 UTC m=+290.312362913" lastFinishedPulling="2023-08-16 15:56:11.462792795 +0000 UTC m=+296.974111818" observedRunningTime="2023-08-16 15:56:12.604646662 +0000 UTC m=+298.115965685" watchObservedRunningTime="2023-08-16 15:56:12.606042904 +0000 UTC m=+298.117361928"
Aug 16 15:56:14 minikube kubelet[1211]: W0816 15:56:14.695993    1211 machine.go:65] Cannot read vendor id correctly, set empty.
Aug 16 15:56:14 minikube kubelet[1211]: E0816 15:56:14.748482    1211 iptables.go:575] "Could not set up iptables canary" err=<
Aug 16 15:56:14 minikube kubelet[1211]:         error creating chain "KUBE-KUBELET-CANARY": exit status 3: ip6tables v1.8.6 (legacy): can't initialize ip6tables table `nat': Table does not exist (do you need to insmod?)
Aug 16 15:56:14 minikube kubelet[1211]:         Perhaps ip6tables or your kernel needs to be upgraded.
Aug 16 15:56:14 minikube kubelet[1211]:  > table=nat chain=KUBE-KUBELET-CANARY
Aug 16 15:57:14 minikube kubelet[1211]: E0816 15:57:14.730131    1211 iptables.go:575] "Could not set up iptables canary" err=<
Aug 16 15:57:14 minikube kubelet[1211]:         error creating chain "KUBE-KUBELET-CANARY": exit status 3: ip6tables v1.8.6 (legacy): can't initialize ip6tables table `nat': Table does not exist (do you need to insmod?)
Aug 16 15:57:14 minikube kubelet[1211]:         Perhaps ip6tables or your kernel needs to be upgraded.
Aug 16 15:57:14 minikube kubelet[1211]:  > table=nat chain=KUBE-KUBELET-CANARY
Aug 16 15:58:14 minikube kubelet[1211]: E0816 15:58:14.732431    1211 iptables.go:575] "Could not set up iptables canary" err=<
Aug 16 15:58:14 minikube kubelet[1211]:         error creating chain "KUBE-KUBELET-CANARY": exit status 3: ip6tables v1.8.6 (legacy): can't initialize ip6tables table `nat': Table does not exist (do you need to insmod?)
Aug 16 15:58:14 minikube kubelet[1211]:         Perhaps ip6tables or your kernel needs to be upgraded.
Aug 16 15:58:14 minikube kubelet[1211]:  > table=nat chain=KUBE-KUBELET-CANARY
Aug 16 15:59:14 minikube kubelet[1211]: E0816 15:59:14.730480    1211 iptables.go:575] "Could not set up iptables canary" err=<
Aug 16 15:59:14 minikube kubelet[1211]:         error creating chain "KUBE-KUBELET-CANARY": exit status 3: ip6tables v1.8.6 (legacy): can't initialize ip6tables table `nat': Table does not exist (do you need to insmod?)
Aug 16 15:59:14 minikube kubelet[1211]:         Perhaps ip6tables or your kernel needs to be upgraded.
Aug 16 15:59:14 minikube kubelet[1211]:  > table=nat chain=KUBE-KUBELET-CANARY

* 
* ==> kubernetes-dashboard [af86ca725b5e] <==
* 2023/08/16 15:58:54 [2023-08-16T15:58:54Z] Incoming HTTP/1.1 GET /api/v1/login/status request from 127.0.0.1: 
2023/08/16 15:58:54 [2023-08-16T15:58:54Z] Outcoming response to 127.0.0.1 with 200 status code
2023/08/16 15:58:54 [2023-08-16T15:58:54Z] Incoming HTTP/1.1 GET /api/v1/service/default?itemsPerPage=10&page=1&sortBy=d,creationTimestamp request from 127.0.0.1: 
2023/08/16 15:58:54 Getting list of all services in the cluster
2023/08/16 15:58:54 [2023-08-16T15:58:54Z] Outcoming response to 127.0.0.1 with 200 status code
2023/08/16 15:58:56 [2023-08-16T15:58:56Z] Incoming HTTP/1.1 GET /api/v1/namespace request from 127.0.0.1: 
2023/08/16 15:58:56 Getting list of namespaces
2023/08/16 15:58:56 [2023-08-16T15:58:56Z] Outcoming response to 127.0.0.1 with 200 status code
2023/08/16 15:58:56 [2023-08-16T15:58:56Z] Incoming HTTP/1.1 GET /api/v1/plugin/config request from 127.0.0.1: 
2023/08/16 15:58:56 [2023-08-16T15:58:56Z] Incoming HTTP/1.1 GET /api/v1/settings/global request from 127.0.0.1: 
2023/08/16 15:58:56 Getting application global configuration
2023/08/16 15:58:56 Application configuration {"serverTime":1692201536728}
2023/08/16 15:58:56 [2023-08-16T15:58:56Z] Outcoming response to 127.0.0.1 with 200 status code
2023/08/16 15:58:56 [2023-08-16T15:58:56Z] Outcoming response to 127.0.0.1 with 200 status code
2023/08/16 15:58:56 [2023-08-16T15:58:56Z] Incoming HTTP/1.1 GET /api/v1/settings/pinner request from 127.0.0.1: 
2023/08/16 15:58:56 [2023-08-16T15:58:56Z] Incoming HTTP/1.1 GET /api/v1/settings/global request from 127.0.0.1: 
2023/08/16 15:58:56 [2023-08-16T15:58:56Z] Outcoming response to 127.0.0.1 with 200 status code
2023/08/16 15:58:56 [2023-08-16T15:58:56Z] Outcoming response to 127.0.0.1 with 200 status code
2023/08/16 15:58:56 [2023-08-16T15:58:56Z] Incoming HTTP/1.1 GET /api/v1/login/status request from 127.0.0.1: 
2023/08/16 15:58:56 [2023-08-16T15:58:56Z] Outcoming response to 127.0.0.1 with 200 status code
2023/08/16 15:58:56 [2023-08-16T15:58:56Z] Incoming HTTP/1.1 GET /api/v1/systembanner request from 127.0.0.1: 
2023/08/16 15:58:56 [2023-08-16T15:58:56Z] Outcoming response to 127.0.0.1 with 200 status code
2023/08/16 15:58:56 [2023-08-16T15:58:56Z] Incoming HTTP/1.1 GET /api/v1/login/status request from 127.0.0.1: 
2023/08/16 15:58:56 [2023-08-16T15:58:56Z] Outcoming response to 127.0.0.1 with 200 status code
2023/08/16 15:58:56 [2023-08-16T15:58:56Z] Incoming HTTP/1.1 GET /api/v1/namespace request from 127.0.0.1: 
2023/08/16 15:58:56 Getting list of namespaces
2023/08/16 15:58:56 [2023-08-16T15:58:56Z] Incoming HTTP/1.1 GET /api/v1/service/default?itemsPerPage=10&page=1&sortBy=d,creationTimestamp request from 127.0.0.1: 
2023/08/16 15:58:56 Getting list of all services in the cluster
2023/08/16 15:58:56 [2023-08-16T15:58:56Z] Outcoming response to 127.0.0.1 with 200 status code
2023/08/16 15:58:56 [2023-08-16T15:58:56Z] Outcoming response to 127.0.0.1 with 200 status code
2023/08/16 15:59:01 [2023-08-16T15:59:01Z] Incoming HTTP/1.1 GET /api/v1/namespace request from 127.0.0.1: 
2023/08/16 15:59:01 Getting list of namespaces
2023/08/16 15:59:01 [2023-08-16T15:59:01Z] Incoming HTTP/1.1 GET /api/v1/service/default?itemsPerPage=10&page=1&sortBy=d,creationTimestamp request from 127.0.0.1: 
2023/08/16 15:59:01 Getting list of all services in the cluster
2023/08/16 15:59:01 [2023-08-16T15:59:01Z] Outcoming response to 127.0.0.1 with 200 status code
2023/08/16 15:59:01 [2023-08-16T15:59:01Z] Outcoming response to 127.0.0.1 with 200 status code
2023/08/16 15:59:06 [2023-08-16T15:59:06Z] Incoming HTTP/1.1 GET /api/v1/namespace request from 127.0.0.1: 
2023/08/16 15:59:06 Getting list of namespaces
2023/08/16 15:59:06 [2023-08-16T15:59:06Z] Incoming HTTP/1.1 GET /api/v1/service/default?itemsPerPage=10&page=1&sortBy=d,creationTimestamp request from 127.0.0.1: 
2023/08/16 15:59:06 Getting list of all services in the cluster
2023/08/16 15:59:06 [2023-08-16T15:59:06Z] Outcoming response to 127.0.0.1 with 200 status code
2023/08/16 15:59:06 [2023-08-16T15:59:06Z] Outcoming response to 127.0.0.1 with 200 status code
2023/08/16 15:59:11 [2023-08-16T15:59:11Z] Incoming HTTP/1.1 GET /api/v1/namespace request from 127.0.0.1: 
2023/08/16 15:59:11 [2023-08-16T15:59:11Z] Incoming HTTP/1.1 GET /api/v1/service/default?itemsPerPage=10&page=1&sortBy=d,creationTimestamp request from 127.0.0.1: 
2023/08/16 15:59:11 Getting list of namespaces
2023/08/16 15:59:11 Getting list of all services in the cluster
2023/08/16 15:59:11 [2023-08-16T15:59:11Z] Outcoming response to 127.0.0.1 with 200 status code
2023/08/16 15:59:11 [2023-08-16T15:59:11Z] Outcoming response to 127.0.0.1 with 200 status code
2023/08/16 15:59:16 [2023-08-16T15:59:16Z] Incoming HTTP/1.1 GET /api/v1/service/default?itemsPerPage=10&page=1&sortBy=d,creationTimestamp request from 127.0.0.1: 
2023/08/16 15:59:16 [2023-08-16T15:59:16Z] Incoming HTTP/1.1 GET /api/v1/namespace request from 127.0.0.1: 
2023/08/16 15:59:16 Getting list of namespaces
2023/08/16 15:59:16 Getting list of all services in the cluster
2023/08/16 15:59:16 [2023-08-16T15:59:16Z] Outcoming response to 127.0.0.1 with 200 status code
2023/08/16 15:59:16 [2023-08-16T15:59:16Z] Outcoming response to 127.0.0.1 with 200 status code
2023/08/16 15:59:21 [2023-08-16T15:59:21Z] Incoming HTTP/1.1 GET /api/v1/namespace request from 127.0.0.1: 
2023/08/16 15:59:21 [2023-08-16T15:59:21Z] Incoming HTTP/1.1 GET /api/v1/service/default?itemsPerPage=10&page=1&sortBy=d,creationTimestamp request from 127.0.0.1: 
2023/08/16 15:59:21 Getting list of all services in the cluster
2023/08/16 15:59:21 Getting list of namespaces
2023/08/16 15:59:21 [2023-08-16T15:59:21Z] Outcoming response to 127.0.0.1 with 200 status code
2023/08/16 15:59:21 [2023-08-16T15:59:21Z] Outcoming response to 127.0.0.1 with 200 status code

* 
* ==> kubernetes-dashboard [e54576a9ff17] <==
* 2023/08/16 10:57:45 [2023-08-16T10:57:45Z] Incoming HTTP/1.1 GET /api/v1/deployment/default?itemsPerPage=10&page=1&sortBy=d,creationTimestamp request from 127.0.0.1: 
2023/08/16 10:57:45 Getting list of all deployments in the cluster
2023/08/16 10:57:45 [2023-08-16T10:57:45Z] Outcoming response to 127.0.0.1 with 200 status code
2023/08/16 10:57:47 [2023-08-16T10:57:47Z] Incoming HTTP/1.1 GET /api/v1/namespace request from 127.0.0.1: 
2023/08/16 10:57:47 Getting list of namespaces
2023/08/16 10:57:47 [2023-08-16T10:57:47Z] Outcoming response to 127.0.0.1 with 200 status code
2023/08/16 10:57:50 [2023-08-16T10:57:50Z] Incoming HTTP/1.1 GET /api/v1/deployment/default?itemsPerPage=10&page=1&sortBy=d,creationTimestamp request from 127.0.0.1: 
2023/08/16 10:57:50 Getting list of all deployments in the cluster
2023/08/16 10:57:50 [2023-08-16T10:57:50Z] Outcoming response to 127.0.0.1 with 200 status code
2023/08/16 10:57:52 [2023-08-16T10:57:52Z] Incoming HTTP/1.1 GET /api/v1/namespace request from 127.0.0.1: 
2023/08/16 10:57:52 Getting list of namespaces
2023/08/16 10:57:52 [2023-08-16T10:57:52Z] Outcoming response to 127.0.0.1 with 200 status code
2023/08/16 10:57:55 [2023-08-16T10:57:55Z] Incoming HTTP/1.1 GET /api/v1/deployment/default?itemsPerPage=10&page=1&sortBy=d,creationTimestamp request from 127.0.0.1: 
2023/08/16 10:57:55 Getting list of all deployments in the cluster
2023/08/16 10:57:55 [2023-08-16T10:57:55Z] Outcoming response to 127.0.0.1 with 200 status code
2023/08/16 10:57:57 [2023-08-16T10:57:57Z] Incoming HTTP/1.1 GET /api/v1/namespace request from 127.0.0.1: 
2023/08/16 10:57:57 Getting list of namespaces
2023/08/16 10:57:57 [2023-08-16T10:57:57Z] Outcoming response to 127.0.0.1 with 200 status code
2023/08/16 10:58:00 [2023-08-16T10:58:00Z] Incoming HTTP/1.1 GET /api/v1/deployment/default?itemsPerPage=10&page=1&sortBy=d,creationTimestamp request from 127.0.0.1: 
2023/08/16 10:58:00 Getting list of all deployments in the cluster
2023/08/16 10:58:00 [2023-08-16T10:58:00Z] Outcoming response to 127.0.0.1 with 200 status code
2023/08/16 10:58:02 [2023-08-16T10:58:02Z] Incoming HTTP/1.1 GET /api/v1/namespace request from 127.0.0.1: 
2023/08/16 10:58:02 Getting list of namespaces
2023/08/16 10:58:02 [2023-08-16T10:58:02Z] Outcoming response to 127.0.0.1 with 200 status code
2023/08/16 10:58:05 [2023-08-16T10:58:05Z] Incoming HTTP/1.1 GET /api/v1/deployment/default?itemsPerPage=10&page=1&sortBy=d,creationTimestamp request from 127.0.0.1: 
2023/08/16 10:58:05 Getting list of all deployments in the cluster
2023/08/16 10:58:05 [2023-08-16T10:58:05Z] Outcoming response to 127.0.0.1 with 200 status code
2023/08/16 10:58:07 [2023-08-16T10:58:07Z] Incoming HTTP/1.1 GET /api/v1/namespace request from 127.0.0.1: 
2023/08/16 10:58:07 Getting list of namespaces
2023/08/16 10:58:07 [2023-08-16T10:58:07Z] Outcoming response to 127.0.0.1 with 200 status code
2023/08/16 10:58:10 [2023-08-16T10:58:10Z] Incoming HTTP/1.1 GET /api/v1/deployment/default?itemsPerPage=10&page=1&sortBy=d,creationTimestamp request from 127.0.0.1: 
2023/08/16 10:58:10 Getting list of all deployments in the cluster
2023/08/16 10:58:10 [2023-08-16T10:58:10Z] Outcoming response to 127.0.0.1 with 200 status code
2023/08/16 10:58:12 [2023-08-16T10:58:12Z] Incoming HTTP/1.1 GET /api/v1/namespace request from 127.0.0.1: 
2023/08/16 10:58:12 Getting list of namespaces
2023/08/16 10:58:12 [2023-08-16T10:58:12Z] Outcoming response to 127.0.0.1 with 200 status code
2023/08/16 10:58:15 [2023-08-16T10:58:15Z] Incoming HTTP/1.1 GET /api/v1/deployment/default?itemsPerPage=10&page=1&sortBy=d,creationTimestamp request from 127.0.0.1: 
2023/08/16 10:58:15 Getting list of all deployments in the cluster
2023/08/16 10:58:15 [2023-08-16T10:58:15Z] Outcoming response to 127.0.0.1 with 200 status code
2023/08/16 10:58:17 [2023-08-16T10:58:17Z] Incoming HTTP/1.1 GET /api/v1/namespace request from 127.0.0.1: 
2023/08/16 10:58:17 Getting list of namespaces
2023/08/16 10:58:17 [2023-08-16T10:58:17Z] Outcoming response to 127.0.0.1 with 200 status code
2023/08/16 10:58:20 [2023-08-16T10:58:20Z] Incoming HTTP/1.1 GET /api/v1/deployment/default?itemsPerPage=10&page=1&sortBy=d,creationTimestamp request from 127.0.0.1: 
2023/08/16 10:58:20 Getting list of all deployments in the cluster
2023/08/16 10:58:20 [2023-08-16T10:58:20Z] Outcoming response to 127.0.0.1 with 200 status code
2023/08/16 10:58:22 [2023-08-16T10:58:22Z] Incoming HTTP/1.1 GET /api/v1/namespace request from 127.0.0.1: 
2023/08/16 10:58:22 Getting list of namespaces
2023/08/16 10:58:22 [2023-08-16T10:58:22Z] Outcoming response to 127.0.0.1 with 200 status code
2023/08/16 10:58:25 [2023-08-16T10:58:25Z] Incoming HTTP/1.1 GET /api/v1/deployment/default?itemsPerPage=10&page=1&sortBy=d,creationTimestamp request from 127.0.0.1: 
2023/08/16 10:58:25 Getting list of all deployments in the cluster
2023/08/16 10:58:25 [2023-08-16T10:58:25Z] Outcoming response to 127.0.0.1 with 200 status code
2023/08/16 10:58:27 [2023-08-16T10:58:27Z] Incoming HTTP/1.1 GET /api/v1/namespace request from 127.0.0.1: 
2023/08/16 10:58:27 Getting list of namespaces
2023/08/16 10:58:27 [2023-08-16T10:58:27Z] Outcoming response to 127.0.0.1 with 200 status code
2023/08/16 10:58:30 [2023-08-16T10:58:30Z] Incoming HTTP/1.1 GET /api/v1/deployment/default?itemsPerPage=10&page=1&sortBy=d,creationTimestamp request from 127.0.0.1: 
2023/08/16 10:58:30 Getting list of all deployments in the cluster
2023/08/16 10:58:30 [2023-08-16T10:58:30Z] Incoming HTTP/1.1 GET /api/v1/namespace request from 127.0.0.1: 
2023/08/16 10:58:30 Getting list of namespaces
2023/08/16 10:58:30 [2023-08-16T10:58:30Z] Outcoming response to 127.0.0.1 with 200 status code
2023/08/16 10:58:30 [2023-08-16T10:58:30Z] Outcoming response to 127.0.0.1 with 200 status code

* 
* ==> storage-provisioner [057ac53ae088] <==
* I0816 15:51:20.352107       1 storage_provisioner.go:116] Initializing the minikube storage provisioner...
F0816 15:51:50.362263       1 main.go:39] error getting server version: Get "https://10.96.0.1:443/version?timeout=32s": dial tcp 10.96.0.1:443: i/o timeout

* 
* ==> storage-provisioner [5aa86e72869d] <==
* I0816 15:52:02.901542       1 storage_provisioner.go:116] Initializing the minikube storage provisioner...
I0816 15:52:02.916149       1 storage_provisioner.go:141] Storage provisioner initialized, now starting service!
I0816 15:52:02.916612       1 leaderelection.go:243] attempting to acquire leader lease kube-system/k8s.io-minikube-hostpath...
I0816 15:52:20.340711       1 leaderelection.go:253] successfully acquired lease kube-system/k8s.io-minikube-hostpath
I0816 15:52:20.341430       1 controller.go:835] Starting provisioner controller k8s.io/minikube-hostpath_minikube_5451407c-83e6-41c9-8c1f-b144235558d8!
I0816 15:52:20.341938       1 event.go:282] Event(v1.ObjectReference{Kind:"Endpoints", Namespace:"kube-system", Name:"k8s.io-minikube-hostpath", UID:"719a2eb2-6dc2-4df9-82bf-fa8050b07ead", APIVersion:"v1", ResourceVersion:"207998", FieldPath:""}): type: 'Normal' reason: 'LeaderElection' minikube_5451407c-83e6-41c9-8c1f-b144235558d8 became leader
I0816 15:52:20.444265       1 controller.go:884] Started provisioner controller k8s.io/minikube-hostpath_minikube_5451407c-83e6-41c9-8c1f-b144235558d8!

